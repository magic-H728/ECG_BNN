{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.model import ECG_XNOR_Full_Bin, ECG_XNOR_Ori\n",
    "from utils.OP import WeightOperation\n",
    "from utils.dataset import Loader\n",
    "from utils.engine import train\n",
    "from utils.save_model import save_model\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "classes_num = 5\n",
    "test_size = 0.2\n",
    "if classes_num == 17:\n",
    "    batch_size = 64\n",
    "    lr = 0.002\n",
    "    seed = 142\n",
    "else:\n",
    "    batch_size = 512\n",
    "    lr = 0.02\n",
    "    seed = 101\n",
    "\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "loader = Loader(batch_size=batch_size, classes_num=classes_num, device=device, test_size=test_size)\n",
    "labels, train_loader, test_loader = loader.loader()\n",
    "# in_channels, out_channels,    kernel_size,     stride,    padding,   pad_value,   pool_size,  pool_stride\n",
    "kernel_size, padding, poolsize =7, 5, 7\n",
    "padding_value = 1\n",
    "A = [[1,           8,           kernel_size,       2,       padding,       padding_value,       poolsize,        2],\n",
    "     [8,          16,           kernel_size,       1,       padding,       padding_value,       poolsize,        2],\n",
    "     [16,         32,           kernel_size,       1,       padding,       padding_value,       poolsize,        2],\n",
    "     [32,         32,           kernel_size,       1,       padding,       padding_value,       poolsize,        2],\n",
    "     [32,         64,           kernel_size,       1,       padding,       padding_value,       poolsize,        2],\n",
    "     [64,         classes_num,  kernel_size,       1,       padding,       padding_value,       poolsize,        2],\n",
    "     ]\n",
    "\n",
    "model = ECG_XNOR_Ori(block1=A[0], block2=A[1], block3=A[2], block4=A[3],\n",
    "                      block5=A[4] if len(A) > 4 else None,\n",
    "                      block6=A[5] if len(A) > 5 else None,\n",
    "                      block7=A[6] if len(A) > 6 else None,\n",
    "                      device=device).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "print(device)\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "ECG_XNOR_Ori (ECG_XNOR_Ori)                   [512, 1, 3600]       [512, 5]             --                   True\n",
       "├─Bn_bin_conv_pool_block_bw (block1)          [512, 1, 3600]       [512, 8, 898]        --                   True\n",
       "│    └─ConstantPad1d (pad)                    [512, 1, 3600]       [512, 1, 3610]       --                   --\n",
       "│    └─BinaryConv1d_bw (conv)                 [512, 1, 3610]       [512, 8, 1802]       56                   True\n",
       "│    └─MaxPool1d (pool)                       [512, 8, 1802]       [512, 8, 898]        --                   --\n",
       "│    └─PReLU (prelu)                          [512, 8, 898]        [512, 8, 898]        1                    True\n",
       "│    └─BatchNorm1d (bn)                       [512, 8, 898]        [512, 8, 898]        16                   True\n",
       "├─Bn_bin_conv_pool_block_baw (block2)         [512, 8, 898]        [512, 16, 448]       --                   True\n",
       "│    └─ConstantPad1d (pad)                    [512, 8, 898]        [512, 8, 908]        --                   --\n",
       "│    └─BinaryConv1d_baw (conv)                [512, 8, 908]        [512, 16, 902]       896                  True\n",
       "│    └─MaxPool1d (pool)                       [512, 16, 902]       [512, 16, 448]       --                   --\n",
       "│    └─PReLU (prelu)                          [512, 16, 448]       [512, 16, 448]       1                    True\n",
       "│    └─BatchNorm1d (bn)                       [512, 16, 448]       [512, 16, 448]       32                   True\n",
       "├─Bn_bin_conv_pool_block_baw (block3)         [512, 16, 448]       [512, 32, 223]       --                   True\n",
       "│    └─ConstantPad1d (pad)                    [512, 16, 448]       [512, 16, 458]       --                   --\n",
       "│    └─BinaryConv1d_baw (conv)                [512, 16, 458]       [512, 32, 452]       3,584                True\n",
       "│    └─MaxPool1d (pool)                       [512, 32, 452]       [512, 32, 223]       --                   --\n",
       "│    └─PReLU (prelu)                          [512, 32, 223]       [512, 32, 223]       1                    True\n",
       "│    └─BatchNorm1d (bn)                       [512, 32, 223]       [512, 32, 223]       64                   True\n",
       "├─Bn_bin_conv_pool_block_baw (block4)         [512, 32, 223]       [512, 32, 111]       --                   True\n",
       "│    └─ConstantPad1d (pad)                    [512, 32, 223]       [512, 32, 233]       --                   --\n",
       "│    └─BinaryConv1d_baw (conv)                [512, 32, 233]       [512, 32, 227]       7,168                True\n",
       "│    └─MaxPool1d (pool)                       [512, 32, 227]       [512, 32, 111]       --                   --\n",
       "│    └─PReLU (prelu)                          [512, 32, 111]       [512, 32, 111]       1                    True\n",
       "│    └─BatchNorm1d (bn)                       [512, 32, 111]       [512, 32, 111]       64                   True\n",
       "├─Bn_bin_conv_pool_block_baw (block5)         [512, 32, 111]       [512, 64, 55]        --                   True\n",
       "│    └─ConstantPad1d (pad)                    [512, 32, 111]       [512, 32, 121]       --                   --\n",
       "│    └─BinaryConv1d_baw (conv)                [512, 32, 121]       [512, 64, 115]       14,336               True\n",
       "│    └─MaxPool1d (pool)                       [512, 64, 115]       [512, 64, 55]        --                   --\n",
       "│    └─PReLU (prelu)                          [512, 64, 55]        [512, 64, 55]        1                    True\n",
       "│    └─BatchNorm1d (bn)                       [512, 64, 55]        [512, 64, 55]        128                  True\n",
       "├─Bn_bin_conv_pool_block_baw (block6)         [512, 64, 55]        [512, 5, 27]         --                   True\n",
       "│    └─ConstantPad1d (pad)                    [512, 64, 55]        [512, 64, 65]        --                   --\n",
       "│    └─BinaryConv1d_baw (conv)                [512, 64, 65]        [512, 5, 59]         2,240                True\n",
       "│    └─MaxPool1d (pool)                       [512, 5, 59]         [512, 5, 27]         --                   --\n",
       "│    └─PReLU (prelu)                          [512, 5, 27]         [512, 5, 27]         1                    True\n",
       "│    └─BatchNorm1d (bn)                       [512, 5, 27]         [512, 5, 27]         10                   True\n",
       "├─Dropout (dropout0)                          [512, 5, 27]         [512, 5, 27]         --                   --\n",
       "=============================================================================================================================\n",
       "Total params: 28,600\n",
       "Trainable params: 28,600\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 3.04\n",
       "=============================================================================================================================\n",
       "Input size (MB): 7.37\n",
       "Forward/backward pass size (MB): 473.58\n",
       "Params size (MB): 0.11\n",
       "Estimated Total Size (MB): 481.07\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model=model,\n",
    "        input_size=(batch_size, 1, 3600),  # make sure this is \"input_size\", not \"input_shape\"\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightOperation = WeightOperation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39d684a66f0402387b5d68180e304dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 5.9461 | train_acc: 0.4160 | test_loss: 3.0218 | test_acc: 0.6770\n",
      "Epoch: 2 | train_loss: 2.2459 | train_acc: 0.6662 | test_loss: 0.9499 | test_acc: 0.7603\n",
      "Epoch: 3 | train_loss: 1.6892 | train_acc: 0.6662 | test_loss: 0.8321 | test_acc: 0.7649\n",
      "Epoch: 4 | train_loss: 1.2959 | train_acc: 0.6983 | test_loss: 0.7366 | test_acc: 0.7855\n",
      "Epoch: 5 | train_loss: 0.9297 | train_acc: 0.7282 | test_loss: 0.5697 | test_acc: 0.7901\n",
      "Epoch: 6 | train_loss: 0.8024 | train_acc: 0.7382 | test_loss: 0.7401 | test_acc: 0.7913\n",
      "Epoch: 7 | train_loss: 0.7237 | train_acc: 0.7673 | test_loss: 0.7499 | test_acc: 0.7778\n",
      "Epoch: 8 | train_loss: 0.6459 | train_acc: 0.7742 | test_loss: 0.6314 | test_acc: 0.7687\n",
      "Epoch: 9 | train_loss: 0.6191 | train_acc: 0.7910 | test_loss: 0.5553 | test_acc: 0.7791\n",
      "Epoch: 10 | train_loss: 0.5920 | train_acc: 0.7986 | test_loss: 0.5918 | test_acc: 0.8056\n",
      "Epoch: 11 | train_loss: 0.5218 | train_acc: 0.8140 | test_loss: 0.4799 | test_acc: 0.8230\n",
      "Epoch: 12 | train_loss: 0.5179 | train_acc: 0.8236 | test_loss: 0.4770 | test_acc: 0.8366\n",
      "Epoch: 13 | train_loss: 0.5202 | train_acc: 0.8335 | test_loss: 0.4280 | test_acc: 0.8482\n",
      "Epoch: 14 | train_loss: 0.4893 | train_acc: 0.8391 | test_loss: 0.4148 | test_acc: 0.8353\n",
      "Epoch: 15 | train_loss: 0.4630 | train_acc: 0.8445 | test_loss: 0.5372 | test_acc: 0.8275\n",
      "Epoch: 16 | train_loss: 0.4428 | train_acc: 0.8540 | test_loss: 0.4361 | test_acc: 0.8527\n",
      "Epoch: 17 | train_loss: 0.4163 | train_acc: 0.8624 | test_loss: 0.4997 | test_acc: 0.8488\n",
      "Epoch: 18 | train_loss: 0.4157 | train_acc: 0.8653 | test_loss: 0.4708 | test_acc: 0.8689\n",
      "Epoch: 19 | train_loss: 0.4087 | train_acc: 0.8598 | test_loss: 0.3975 | test_acc: 0.8585\n",
      "Epoch: 20 | train_loss: 0.3975 | train_acc: 0.8732 | test_loss: 0.4021 | test_acc: 0.8656\n",
      "Epoch: 21 | train_loss: 0.3837 | train_acc: 0.8765 | test_loss: 0.4069 | test_acc: 0.8637\n",
      "Epoch: 22 | train_loss: 0.3639 | train_acc: 0.8766 | test_loss: 0.6330 | test_acc: 0.8663\n",
      "Epoch: 23 | train_loss: 0.3375 | train_acc: 0.8871 | test_loss: 0.3345 | test_acc: 0.8792\n",
      "Epoch: 24 | train_loss: 0.3489 | train_acc: 0.8891 | test_loss: 0.4872 | test_acc: 0.8766\n",
      "Epoch: 25 | train_loss: 0.4081 | train_acc: 0.8682 | test_loss: 0.5579 | test_acc: 0.8514\n",
      "Epoch: 26 | train_loss: 0.3489 | train_acc: 0.8871 | test_loss: 0.4249 | test_acc: 0.8786\n",
      "Epoch: 27 | train_loss: 0.3448 | train_acc: 0.8845 | test_loss: 0.3614 | test_acc: 0.8430\n",
      "Epoch: 28 | train_loss: 0.3614 | train_acc: 0.8857 | test_loss: 0.3735 | test_acc: 0.8714\n",
      "Epoch: 29 | train_loss: 0.3402 | train_acc: 0.8795 | test_loss: 0.3205 | test_acc: 0.8643\n",
      "Epoch: 30 | train_loss: 0.3552 | train_acc: 0.8858 | test_loss: 0.2858 | test_acc: 0.8798\n",
      "Epoch: 31 | train_loss: 0.3335 | train_acc: 0.8858 | test_loss: 0.4197 | test_acc: 0.8572\n",
      "Epoch: 32 | train_loss: 0.3493 | train_acc: 0.8840 | test_loss: 0.3103 | test_acc: 0.8676\n",
      "Epoch: 33 | train_loss: 0.3582 | train_acc: 0.8811 | test_loss: 0.3761 | test_acc: 0.8766\n",
      "Epoch: 34 | train_loss: 0.3543 | train_acc: 0.8892 | test_loss: 0.3338 | test_acc: 0.8837\n",
      "Epoch: 35 | train_loss: 0.3187 | train_acc: 0.8945 | test_loss: 0.3917 | test_acc: 0.8766\n",
      "Epoch: 36 | train_loss: 0.3337 | train_acc: 0.8949 | test_loss: 0.3940 | test_acc: 0.8889\n",
      "Epoch: 37 | train_loss: 0.3271 | train_acc: 0.8873 | test_loss: 0.3201 | test_acc: 0.8837\n",
      "Epoch: 38 | train_loss: 0.3138 | train_acc: 0.9005 | test_loss: 0.3015 | test_acc: 0.8960\n",
      "Epoch: 39 | train_loss: 0.2996 | train_acc: 0.8979 | test_loss: 0.3560 | test_acc: 0.8941\n",
      "Epoch: 40 | train_loss: 0.3143 | train_acc: 0.8978 | test_loss: 0.3115 | test_acc: 0.8986\n",
      "Epoch: 41 | train_loss: 0.2950 | train_acc: 0.9004 | test_loss: 0.3205 | test_acc: 0.8999\n",
      "Epoch: 42 | train_loss: 0.3077 | train_acc: 0.9007 | test_loss: 0.5533 | test_acc: 0.8740\n",
      "Epoch: 43 | train_loss: 0.3099 | train_acc: 0.8955 | test_loss: 0.5571 | test_acc: 0.7558\n",
      "Epoch: 44 | train_loss: 0.3245 | train_acc: 0.8974 | test_loss: 0.3180 | test_acc: 0.8844\n",
      "Epoch: 45 | train_loss: 0.2981 | train_acc: 0.8974 | test_loss: 0.4247 | test_acc: 0.8850\n",
      "Epoch: 46 | train_loss: 0.2895 | train_acc: 0.9063 | test_loss: 0.4802 | test_acc: 0.8928\n",
      "Epoch: 47 | train_loss: 0.2932 | train_acc: 0.9063 | test_loss: 0.3181 | test_acc: 0.8895\n",
      "Epoch: 48 | train_loss: 0.2662 | train_acc: 0.9120 | test_loss: 0.3123 | test_acc: 0.8953\n",
      "Epoch: 49 | train_loss: 0.2731 | train_acc: 0.9099 | test_loss: 0.2774 | test_acc: 0.8921\n",
      "Epoch: 50 | train_loss: 0.3100 | train_acc: 0.9071 | test_loss: 0.3000 | test_acc: 0.8921\n",
      "Epoch: 51 | train_loss: 0.3142 | train_acc: 0.8960 | test_loss: 0.3314 | test_acc: 0.8902\n",
      "Epoch: 52 | train_loss: 0.2832 | train_acc: 0.9055 | test_loss: 0.2899 | test_acc: 0.8850\n",
      "Epoch: 53 | train_loss: 0.2816 | train_acc: 0.9062 | test_loss: 0.2757 | test_acc: 0.8844\n",
      "Epoch: 54 | train_loss: 0.2778 | train_acc: 0.9086 | test_loss: 0.2462 | test_acc: 0.9076\n",
      "Epoch: 55 | train_loss: 0.3097 | train_acc: 0.9055 | test_loss: 0.4981 | test_acc: 0.8734\n",
      "Epoch: 56 | train_loss: 0.3054 | train_acc: 0.9060 | test_loss: 0.2473 | test_acc: 0.9005\n",
      "Epoch: 57 | train_loss: 0.2835 | train_acc: 0.9071 | test_loss: 0.3800 | test_acc: 0.8876\n",
      "Epoch: 58 | train_loss: 0.2894 | train_acc: 0.9021 | test_loss: 0.2412 | test_acc: 0.8999\n",
      "Epoch: 59 | train_loss: 0.2770 | train_acc: 0.9097 | test_loss: 0.2733 | test_acc: 0.8960\n",
      "Epoch: 60 | train_loss: 0.2663 | train_acc: 0.9147 | test_loss: 0.2855 | test_acc: 0.8941\n",
      "Epoch: 61 | train_loss: 0.2612 | train_acc: 0.9107 | test_loss: 0.4672 | test_acc: 0.9018\n",
      "Epoch: 62 | train_loss: 0.2599 | train_acc: 0.9131 | test_loss: 0.2429 | test_acc: 0.9083\n",
      "Epoch: 63 | train_loss: 0.2612 | train_acc: 0.9125 | test_loss: 0.3296 | test_acc: 0.9076\n",
      "Epoch: 64 | train_loss: 0.2607 | train_acc: 0.9141 | test_loss: 0.3357 | test_acc: 0.8727\n",
      "Epoch: 65 | train_loss: 0.2456 | train_acc: 0.9199 | test_loss: 0.4690 | test_acc: 0.9121\n",
      "Epoch: 66 | train_loss: 0.2735 | train_acc: 0.9097 | test_loss: 0.2879 | test_acc: 0.8947\n",
      "Epoch: 67 | train_loss: 0.2707 | train_acc: 0.9123 | test_loss: 0.2599 | test_acc: 0.8902\n",
      "Epoch: 68 | train_loss: 0.2940 | train_acc: 0.9105 | test_loss: 0.3440 | test_acc: 0.9063\n",
      "Epoch: 69 | train_loss: 0.2739 | train_acc: 0.9094 | test_loss: 0.2927 | test_acc: 0.8992\n",
      "Epoch: 70 | train_loss: 0.2580 | train_acc: 0.9163 | test_loss: 0.2995 | test_acc: 0.8973\n",
      "Epoch: 71 | train_loss: 0.2612 | train_acc: 0.9175 | test_loss: 0.3297 | test_acc: 0.9070\n",
      "Epoch: 72 | train_loss: 0.2848 | train_acc: 0.9083 | test_loss: 0.3751 | test_acc: 0.8921\n",
      "Epoch: 73 | train_loss: 0.2920 | train_acc: 0.9118 | test_loss: 0.3115 | test_acc: 0.8999\n",
      "Epoch: 74 | train_loss: 0.2738 | train_acc: 0.9060 | test_loss: 0.3844 | test_acc: 0.9044\n",
      "Epoch: 75 | train_loss: 0.2573 | train_acc: 0.9130 | test_loss: 0.3581 | test_acc: 0.8986\n",
      "Epoch: 76 | train_loss: 0.2587 | train_acc: 0.9180 | test_loss: 0.2404 | test_acc: 0.9121\n",
      "Epoch: 77 | train_loss: 0.2618 | train_acc: 0.9131 | test_loss: 0.3132 | test_acc: 0.8979\n",
      "Epoch: 78 | train_loss: 0.2639 | train_acc: 0.9155 | test_loss: 0.3726 | test_acc: 0.8928\n",
      "Epoch: 79 | train_loss: 0.2691 | train_acc: 0.9092 | test_loss: 0.4320 | test_acc: 0.8346\n",
      "Epoch: 80 | train_loss: 0.2680 | train_acc: 0.9110 | test_loss: 0.2434 | test_acc: 0.9037\n",
      "Epoch: 81 | train_loss: 0.2943 | train_acc: 0.9002 | test_loss: 0.5318 | test_acc: 0.8908\n",
      "Epoch: 82 | train_loss: 0.2721 | train_acc: 0.9134 | test_loss: 0.2866 | test_acc: 0.9012\n",
      "Epoch: 83 | train_loss: 0.2472 | train_acc: 0.9181 | test_loss: 0.2243 | test_acc: 0.9050\n",
      "Epoch: 84 | train_loss: 0.2570 | train_acc: 0.9209 | test_loss: 0.3777 | test_acc: 0.9063\n",
      "Epoch: 85 | train_loss: 0.2472 | train_acc: 0.9188 | test_loss: 0.2323 | test_acc: 0.9109\n",
      "Epoch: 86 | train_loss: 0.2534 | train_acc: 0.9183 | test_loss: 0.4383 | test_acc: 0.9102\n",
      "Epoch: 87 | train_loss: 0.2373 | train_acc: 0.9249 | test_loss: 0.2963 | test_acc: 0.8947\n",
      "Epoch: 88 | train_loss: 0.2333 | train_acc: 0.9228 | test_loss: 0.3008 | test_acc: 0.9044\n",
      "Epoch: 89 | train_loss: 0.2535 | train_acc: 0.9226 | test_loss: 0.3045 | test_acc: 0.8895\n",
      "Epoch: 90 | train_loss: 0.2841 | train_acc: 0.9097 | test_loss: 0.2953 | test_acc: 0.8818\n",
      "Epoch: 91 | train_loss: 0.2661 | train_acc: 0.9130 | test_loss: 0.3014 | test_acc: 0.9076\n",
      "Epoch: 92 | train_loss: 0.2489 | train_acc: 0.9209 | test_loss: 0.2698 | test_acc: 0.9147\n",
      "Epoch: 93 | train_loss: 0.2592 | train_acc: 0.9189 | test_loss: 0.3129 | test_acc: 0.9057\n",
      "Epoch: 94 | train_loss: 0.2385 | train_acc: 0.9226 | test_loss: 0.3134 | test_acc: 0.8889\n",
      "Epoch: 95 | train_loss: 0.2896 | train_acc: 0.9071 | test_loss: 0.3203 | test_acc: 0.8986\n",
      "Epoch: 96 | train_loss: 0.2787 | train_acc: 0.9105 | test_loss: 0.3225 | test_acc: 0.8863\n",
      "Epoch: 97 | train_loss: 0.2356 | train_acc: 0.9180 | test_loss: 0.3520 | test_acc: 0.9031\n",
      "Epoch: 98 | train_loss: 0.2436 | train_acc: 0.9238 | test_loss: 0.3064 | test_acc: 0.9115\n",
      "Epoch: 99 | train_loss: 0.2759 | train_acc: 0.9139 | test_loss: 0.5059 | test_acc: 0.8695\n",
      "Epoch: 100 | train_loss: 0.2510 | train_acc: 0.9205 | test_loss: 0.2615 | test_acc: 0.8928\n",
      "Epoch: 101 | train_loss: 0.2428 | train_acc: 0.9201 | test_loss: 0.2818 | test_acc: 0.9218\n",
      "Epoch: 102 | train_loss: 0.2388 | train_acc: 0.9278 | test_loss: 0.2308 | test_acc: 0.9160\n",
      "Epoch: 103 | train_loss: 0.2243 | train_acc: 0.9251 | test_loss: 0.4367 | test_acc: 0.9063\n",
      "Epoch: 104 | train_loss: 0.2255 | train_acc: 0.9293 | test_loss: 0.2818 | test_acc: 0.9018\n",
      "Epoch: 105 | train_loss: 0.2415 | train_acc: 0.9228 | test_loss: 0.2666 | test_acc: 0.8986\n",
      "Epoch: 106 | train_loss: 0.2613 | train_acc: 0.9193 | test_loss: 0.2415 | test_acc: 0.9018\n",
      "Epoch: 107 | train_loss: 0.2488 | train_acc: 0.9178 | test_loss: 0.5271 | test_acc: 0.9031\n",
      "Epoch: 108 | train_loss: 0.2398 | train_acc: 0.9230 | test_loss: 0.2350 | test_acc: 0.9160\n",
      "Epoch: 109 | train_loss: 0.2468 | train_acc: 0.9170 | test_loss: 0.3544 | test_acc: 0.9167\n",
      "Epoch: 110 | train_loss: 0.2581 | train_acc: 0.9209 | test_loss: 0.2695 | test_acc: 0.9134\n",
      "Epoch: 111 | train_loss: 0.2504 | train_acc: 0.9226 | test_loss: 0.2646 | test_acc: 0.9276\n",
      "Epoch: 112 | train_loss: 0.2425 | train_acc: 0.9180 | test_loss: 0.2943 | test_acc: 0.9257\n",
      "Epoch: 113 | train_loss: 0.2455 | train_acc: 0.9197 | test_loss: 0.3187 | test_acc: 0.9115\n",
      "Epoch: 114 | train_loss: 0.2357 | train_acc: 0.9230 | test_loss: 0.3343 | test_acc: 0.9037\n",
      "Epoch: 115 | train_loss: 0.2247 | train_acc: 0.9257 | test_loss: 0.2007 | test_acc: 0.9154\n",
      "Epoch: 116 | train_loss: 0.2623 | train_acc: 0.9254 | test_loss: 0.4201 | test_acc: 0.8941\n",
      "Epoch: 117 | train_loss: 0.2610 | train_acc: 0.9133 | test_loss: 0.2998 | test_acc: 0.9147\n",
      "Epoch: 118 | train_loss: 0.2569 | train_acc: 0.9160 | test_loss: 0.3233 | test_acc: 0.9186\n",
      "Epoch: 119 | train_loss: 0.2287 | train_acc: 0.9260 | test_loss: 0.1858 | test_acc: 0.9257\n",
      "Epoch: 120 | train_loss: 0.2272 | train_acc: 0.9265 | test_loss: 0.2298 | test_acc: 0.9244\n",
      "Epoch: 121 | train_loss: 0.2162 | train_acc: 0.9296 | test_loss: 0.2158 | test_acc: 0.9115\n",
      "Epoch: 122 | train_loss: 0.2348 | train_acc: 0.9280 | test_loss: 0.2249 | test_acc: 0.9186\n",
      "Epoch: 123 | train_loss: 0.2468 | train_acc: 0.9196 | test_loss: 0.2432 | test_acc: 0.9154\n",
      "Epoch: 124 | train_loss: 0.2538 | train_acc: 0.9236 | test_loss: 0.3455 | test_acc: 0.9096\n",
      "Epoch: 125 | train_loss: 0.2375 | train_acc: 0.9239 | test_loss: 0.2691 | test_acc: 0.9147\n",
      "Epoch: 126 | train_loss: 0.2215 | train_acc: 0.9265 | test_loss: 0.2311 | test_acc: 0.9128\n",
      "Epoch: 127 | train_loss: 0.2200 | train_acc: 0.9288 | test_loss: 0.1913 | test_acc: 0.9193\n",
      "Epoch: 128 | train_loss: 0.2214 | train_acc: 0.9252 | test_loss: 0.2613 | test_acc: 0.9096\n",
      "Epoch: 129 | train_loss: 0.2463 | train_acc: 0.9215 | test_loss: 0.3325 | test_acc: 0.9218\n",
      "Epoch: 130 | train_loss: 0.2394 | train_acc: 0.9204 | test_loss: 0.3430 | test_acc: 0.9005\n",
      "Epoch: 131 | train_loss: 0.2227 | train_acc: 0.9264 | test_loss: 0.2278 | test_acc: 0.9205\n",
      "Epoch: 132 | train_loss: 0.2349 | train_acc: 0.9257 | test_loss: 0.2321 | test_acc: 0.9296\n",
      "Epoch: 133 | train_loss: 0.2197 | train_acc: 0.9299 | test_loss: 0.2164 | test_acc: 0.9289\n",
      "Epoch: 134 | train_loss: 0.2278 | train_acc: 0.9318 | test_loss: 0.2310 | test_acc: 0.9212\n",
      "Epoch: 135 | train_loss: 0.2357 | train_acc: 0.9289 | test_loss: 0.3619 | test_acc: 0.9012\n",
      "Epoch: 136 | train_loss: 0.2301 | train_acc: 0.9249 | test_loss: 0.1917 | test_acc: 0.9264\n",
      "Epoch: 137 | train_loss: 0.2231 | train_acc: 0.9312 | test_loss: 0.2295 | test_acc: 0.9335\n",
      "Epoch: 138 | train_loss: 0.2343 | train_acc: 0.9259 | test_loss: 0.3454 | test_acc: 0.9076\n",
      "Epoch: 139 | train_loss: 0.2350 | train_acc: 0.9288 | test_loss: 0.2004 | test_acc: 0.9289\n",
      "Epoch: 140 | train_loss: 0.2328 | train_acc: 0.9273 | test_loss: 0.4927 | test_acc: 0.8605\n",
      "Epoch: 141 | train_loss: 0.2718 | train_acc: 0.9178 | test_loss: 0.2294 | test_acc: 0.9154\n",
      "Epoch: 142 | train_loss: 0.2371 | train_acc: 0.9238 | test_loss: 0.2866 | test_acc: 0.9167\n",
      "Epoch: 143 | train_loss: 0.2288 | train_acc: 0.9246 | test_loss: 0.2482 | test_acc: 0.9251\n",
      "Epoch: 144 | train_loss: 0.2379 | train_acc: 0.9257 | test_loss: 0.2495 | test_acc: 0.9115\n",
      "Epoch: 145 | train_loss: 0.2296 | train_acc: 0.9239 | test_loss: 0.2978 | test_acc: 0.9096\n",
      "Epoch: 146 | train_loss: 0.2499 | train_acc: 0.9215 | test_loss: 0.2199 | test_acc: 0.9121\n",
      "Epoch: 147 | train_loss: 0.2241 | train_acc: 0.9270 | test_loss: 0.1887 | test_acc: 0.9276\n",
      "Epoch: 148 | train_loss: 0.2083 | train_acc: 0.9301 | test_loss: 0.2977 | test_acc: 0.9147\n",
      "Epoch: 149 | train_loss: 0.2190 | train_acc: 0.9281 | test_loss: 0.2087 | test_acc: 0.9186\n",
      "Epoch: 150 | train_loss: 0.2207 | train_acc: 0.9294 | test_loss: 0.2698 | test_acc: 0.9315\n",
      "Epoch: 151 | train_loss: 0.2038 | train_acc: 0.9346 | test_loss: 0.2357 | test_acc: 0.9160\n",
      "Epoch: 152 | train_loss: 0.1944 | train_acc: 0.9377 | test_loss: 0.2834 | test_acc: 0.9276\n",
      "Epoch: 153 | train_loss: 0.2625 | train_acc: 0.9267 | test_loss: 0.3250 | test_acc: 0.9115\n",
      "Epoch: 154 | train_loss: 0.2818 | train_acc: 0.9102 | test_loss: 0.2809 | test_acc: 0.9070\n",
      "Epoch: 155 | train_loss: 0.2408 | train_acc: 0.9197 | test_loss: 0.2483 | test_acc: 0.9193\n",
      "Epoch: 156 | train_loss: 0.2135 | train_acc: 0.9317 | test_loss: 0.3605 | test_acc: 0.9231\n",
      "Epoch: 157 | train_loss: 0.2340 | train_acc: 0.9226 | test_loss: 0.2124 | test_acc: 0.9180\n",
      "Epoch: 158 | train_loss: 0.2169 | train_acc: 0.9322 | test_loss: 0.2821 | test_acc: 0.9335\n",
      "Epoch: 159 | train_loss: 0.2159 | train_acc: 0.9333 | test_loss: 0.2387 | test_acc: 0.9199\n",
      "Epoch: 160 | train_loss: 0.2019 | train_acc: 0.9348 | test_loss: 0.2434 | test_acc: 0.9205\n",
      "Epoch: 161 | train_loss: 0.2284 | train_acc: 0.9304 | test_loss: 0.2173 | test_acc: 0.9238\n",
      "Epoch: 162 | train_loss: 0.2227 | train_acc: 0.9312 | test_loss: 0.2194 | test_acc: 0.9251\n",
      "Epoch: 163 | train_loss: 0.2180 | train_acc: 0.9317 | test_loss: 0.1999 | test_acc: 0.9218\n",
      "Epoch: 164 | train_loss: 0.2162 | train_acc: 0.9327 | test_loss: 0.1851 | test_acc: 0.9296\n",
      "Epoch: 165 | train_loss: 0.2128 | train_acc: 0.9291 | test_loss: 0.2633 | test_acc: 0.9199\n",
      "Epoch: 166 | train_loss: 0.2233 | train_acc: 0.9307 | test_loss: 0.3884 | test_acc: 0.9276\n",
      "Epoch: 167 | train_loss: 0.2167 | train_acc: 0.9369 | test_loss: 0.2234 | test_acc: 0.9193\n",
      "Epoch: 168 | train_loss: 0.2386 | train_acc: 0.9244 | test_loss: 0.2326 | test_acc: 0.9096\n",
      "Epoch: 169 | train_loss: 0.2224 | train_acc: 0.9323 | test_loss: 0.2250 | test_acc: 0.9193\n",
      "Epoch: 170 | train_loss: 0.2573 | train_acc: 0.9222 | test_loss: 0.3682 | test_acc: 0.9121\n",
      "Epoch: 171 | train_loss: 0.2506 | train_acc: 0.9244 | test_loss: 0.2288 | test_acc: 0.9244\n",
      "Epoch: 172 | train_loss: 0.2853 | train_acc: 0.9097 | test_loss: 0.3577 | test_acc: 0.9037\n",
      "Epoch: 173 | train_loss: 0.2433 | train_acc: 0.9249 | test_loss: 0.2500 | test_acc: 0.9205\n",
      "Epoch: 174 | train_loss: 0.2420 | train_acc: 0.9188 | test_loss: 0.2396 | test_acc: 0.9231\n",
      "Epoch: 175 | train_loss: 0.2195 | train_acc: 0.9285 | test_loss: 0.2045 | test_acc: 0.9193\n",
      "Epoch: 176 | train_loss: 0.1977 | train_acc: 0.9343 | test_loss: 0.2053 | test_acc: 0.9244\n",
      "Epoch: 177 | train_loss: 0.2183 | train_acc: 0.9301 | test_loss: 0.2037 | test_acc: 0.9322\n",
      "Epoch: 178 | train_loss: 0.2086 | train_acc: 0.9322 | test_loss: 0.1721 | test_acc: 0.9354\n",
      "Epoch: 179 | train_loss: 0.2134 | train_acc: 0.9333 | test_loss: 0.2155 | test_acc: 0.9270\n",
      "Epoch: 180 | train_loss: 0.2062 | train_acc: 0.9314 | test_loss: 0.2201 | test_acc: 0.9348\n",
      "Epoch: 181 | train_loss: 0.2266 | train_acc: 0.9344 | test_loss: 0.2065 | test_acc: 0.9238\n",
      "Epoch: 182 | train_loss: 0.2768 | train_acc: 0.9126 | test_loss: 0.3054 | test_acc: 0.9115\n",
      "Epoch: 183 | train_loss: 0.2440 | train_acc: 0.9260 | test_loss: 0.3139 | test_acc: 0.9257\n",
      "Epoch: 184 | train_loss: 0.2484 | train_acc: 0.9184 | test_loss: 0.3239 | test_acc: 0.8870\n",
      "Epoch: 185 | train_loss: 0.2465 | train_acc: 0.9252 | test_loss: 0.5651 | test_acc: 0.8702\n",
      "Epoch: 186 | train_loss: 0.2307 | train_acc: 0.9320 | test_loss: 0.2332 | test_acc: 0.9154\n",
      "Epoch: 187 | train_loss: 0.2511 | train_acc: 0.9254 | test_loss: 0.1928 | test_acc: 0.9264\n",
      "Epoch: 188 | train_loss: 0.2128 | train_acc: 0.9331 | test_loss: 0.1836 | test_acc: 0.9270\n",
      "Epoch: 189 | train_loss: 0.2089 | train_acc: 0.9373 | test_loss: 0.2540 | test_acc: 0.9238\n",
      "Epoch: 190 | train_loss: 0.2074 | train_acc: 0.9336 | test_loss: 0.2105 | test_acc: 0.9289\n",
      "Epoch: 191 | train_loss: 0.2206 | train_acc: 0.9299 | test_loss: 0.3661 | test_acc: 0.8999\n",
      "Epoch: 192 | train_loss: 0.2331 | train_acc: 0.9307 | test_loss: 0.2714 | test_acc: 0.9302\n",
      "Epoch: 193 | train_loss: 0.2217 | train_acc: 0.9349 | test_loss: 0.1985 | test_acc: 0.9276\n",
      "Epoch: 194 | train_loss: 0.2106 | train_acc: 0.9294 | test_loss: 0.2871 | test_acc: 0.9354\n",
      "Epoch: 195 | train_loss: 0.1952 | train_acc: 0.9346 | test_loss: 0.1876 | test_acc: 0.9315\n",
      "Epoch: 196 | train_loss: 0.1926 | train_acc: 0.9396 | test_loss: 0.1544 | test_acc: 0.9360\n",
      "Epoch: 197 | train_loss: 0.2024 | train_acc: 0.9377 | test_loss: 0.2531 | test_acc: 0.9322\n",
      "Epoch: 198 | train_loss: 0.2119 | train_acc: 0.9320 | test_loss: 0.2049 | test_acc: 0.9276\n",
      "Epoch: 199 | train_loss: 0.2045 | train_acc: 0.9370 | test_loss: 0.1792 | test_acc: 0.9315\n",
      "Epoch: 200 | train_loss: 0.2148 | train_acc: 0.9339 | test_loss: 0.2145 | test_acc: 0.9251\n",
      "Epoch: 201 | train_loss: 0.2000 | train_acc: 0.9325 | test_loss: 0.1997 | test_acc: 0.9193\n",
      "Epoch: 202 | train_loss: 0.1844 | train_acc: 0.9369 | test_loss: 0.3036 | test_acc: 0.9205\n",
      "Epoch: 203 | train_loss: 0.1985 | train_acc: 0.9386 | test_loss: 0.2084 | test_acc: 0.9264\n",
      "Epoch: 204 | train_loss: 0.2112 | train_acc: 0.9386 | test_loss: 0.1740 | test_acc: 0.9406\n",
      "Epoch: 205 | train_loss: 0.2235 | train_acc: 0.9356 | test_loss: 0.2260 | test_acc: 0.9257\n",
      "Epoch: 206 | train_loss: 0.2409 | train_acc: 0.9197 | test_loss: 0.2742 | test_acc: 0.9167\n",
      "Epoch: 207 | train_loss: 0.2139 | train_acc: 0.9327 | test_loss: 0.2092 | test_acc: 0.9315\n",
      "Epoch: 208 | train_loss: 0.2025 | train_acc: 0.9375 | test_loss: 0.5558 | test_acc: 0.9244\n",
      "Epoch: 209 | train_loss: 0.2099 | train_acc: 0.9328 | test_loss: 0.3108 | test_acc: 0.9289\n",
      "Epoch: 210 | train_loss: 0.2065 | train_acc: 0.9360 | test_loss: 0.5738 | test_acc: 0.9341\n",
      "Epoch: 211 | train_loss: 0.2042 | train_acc: 0.9420 | test_loss: 0.2429 | test_acc: 0.9231\n",
      "Epoch: 212 | train_loss: 0.1860 | train_acc: 0.9398 | test_loss: 0.2129 | test_acc: 0.9231\n",
      "Epoch: 213 | train_loss: 0.2033 | train_acc: 0.9388 | test_loss: 0.2250 | test_acc: 0.9335\n",
      "Epoch: 214 | train_loss: 0.2150 | train_acc: 0.9346 | test_loss: 0.2056 | test_acc: 0.9367\n",
      "Epoch: 215 | train_loss: 0.2316 | train_acc: 0.9222 | test_loss: 0.1907 | test_acc: 0.9328\n",
      "Epoch: 216 | train_loss: 0.1998 | train_acc: 0.9356 | test_loss: 0.3326 | test_acc: 0.9283\n",
      "Epoch: 217 | train_loss: 0.2245 | train_acc: 0.9365 | test_loss: 0.2294 | test_acc: 0.9128\n",
      "Epoch: 218 | train_loss: 0.2072 | train_acc: 0.9351 | test_loss: 0.2212 | test_acc: 0.9160\n",
      "Epoch: 219 | train_loss: 0.1930 | train_acc: 0.9383 | test_loss: 0.2097 | test_acc: 0.9322\n",
      "Epoch: 220 | train_loss: 0.1956 | train_acc: 0.9427 | test_loss: 0.5877 | test_acc: 0.9070\n",
      "Epoch: 221 | train_loss: 0.2086 | train_acc: 0.9322 | test_loss: 0.1983 | test_acc: 0.9289\n",
      "Epoch: 222 | train_loss: 0.2002 | train_acc: 0.9380 | test_loss: 0.4412 | test_acc: 0.9270\n",
      "Epoch: 223 | train_loss: 0.1917 | train_acc: 0.9372 | test_loss: 0.3230 | test_acc: 0.9348\n",
      "Epoch: 224 | train_loss: 0.2241 | train_acc: 0.9360 | test_loss: 0.2912 | test_acc: 0.9044\n",
      "Epoch: 225 | train_loss: 0.2322 | train_acc: 0.9239 | test_loss: 0.2613 | test_acc: 0.9018\n",
      "Epoch: 226 | train_loss: 0.2339 | train_acc: 0.9230 | test_loss: 0.1763 | test_acc: 0.9360\n",
      "Epoch: 227 | train_loss: 0.2161 | train_acc: 0.9325 | test_loss: 0.2342 | test_acc: 0.9186\n",
      "Epoch: 228 | train_loss: 0.1943 | train_acc: 0.9367 | test_loss: 0.2762 | test_acc: 0.9289\n",
      "Epoch: 229 | train_loss: 0.2039 | train_acc: 0.9367 | test_loss: 0.2031 | test_acc: 0.9360\n",
      "Epoch: 230 | train_loss: 0.1945 | train_acc: 0.9388 | test_loss: 0.2557 | test_acc: 0.9231\n",
      "Epoch: 231 | train_loss: 0.1927 | train_acc: 0.9407 | test_loss: 0.2258 | test_acc: 0.9283\n",
      "Epoch: 232 | train_loss: 0.2357 | train_acc: 0.9309 | test_loss: 0.3000 | test_acc: 0.8953\n",
      "Epoch: 233 | train_loss: 0.2397 | train_acc: 0.9223 | test_loss: 0.2898 | test_acc: 0.9309\n",
      "Epoch: 234 | train_loss: 0.2241 | train_acc: 0.9254 | test_loss: 0.3430 | test_acc: 0.8798\n",
      "Epoch: 235 | train_loss: 0.2203 | train_acc: 0.9288 | test_loss: 0.2368 | test_acc: 0.9154\n",
      "Epoch: 236 | train_loss: 0.2077 | train_acc: 0.9338 | test_loss: 0.2219 | test_acc: 0.9380\n",
      "Epoch: 237 | train_loss: 0.1823 | train_acc: 0.9399 | test_loss: 0.3435 | test_acc: 0.9244\n",
      "Epoch: 238 | train_loss: 0.1968 | train_acc: 0.9378 | test_loss: 0.1863 | test_acc: 0.9367\n",
      "Epoch: 239 | train_loss: 0.1995 | train_acc: 0.9354 | test_loss: 0.2591 | test_acc: 0.9270\n",
      "Epoch: 240 | train_loss: 0.2072 | train_acc: 0.9456 | test_loss: 0.2103 | test_acc: 0.9309\n",
      "Epoch: 241 | train_loss: 0.1890 | train_acc: 0.9346 | test_loss: 0.2150 | test_acc: 0.9367\n",
      "Epoch: 242 | train_loss: 0.1928 | train_acc: 0.9378 | test_loss: 0.3283 | test_acc: 0.9231\n",
      "Epoch: 243 | train_loss: 0.1997 | train_acc: 0.9383 | test_loss: 0.1730 | test_acc: 0.9354\n",
      "Epoch: 244 | train_loss: 0.2098 | train_acc: 0.9412 | test_loss: 0.2072 | test_acc: 0.9302\n",
      "Epoch: 245 | train_loss: 0.2056 | train_acc: 0.9325 | test_loss: 0.2154 | test_acc: 0.9238\n",
      "Epoch: 246 | train_loss: 0.2022 | train_acc: 0.9362 | test_loss: 0.4163 | test_acc: 0.9270\n",
      "Epoch: 247 | train_loss: 0.1771 | train_acc: 0.9461 | test_loss: 0.1772 | test_acc: 0.9270\n",
      "Epoch: 248 | train_loss: 0.1897 | train_acc: 0.9435 | test_loss: 0.2003 | test_acc: 0.9322\n",
      "Epoch: 249 | train_loss: 0.1809 | train_acc: 0.9444 | test_loss: 0.2589 | test_acc: 0.9335\n",
      "Epoch: 250 | train_loss: 0.1921 | train_acc: 0.9446 | test_loss: 0.2129 | test_acc: 0.9309\n",
      "Epoch: 251 | train_loss: 0.1985 | train_acc: 0.9391 | test_loss: 0.2985 | test_acc: 0.9328\n",
      "Epoch: 252 | train_loss: 0.1902 | train_acc: 0.9394 | test_loss: 0.1895 | test_acc: 0.9341\n",
      "Epoch: 253 | train_loss: 0.1820 | train_acc: 0.9404 | test_loss: 0.2105 | test_acc: 0.9296\n",
      "Epoch: 254 | train_loss: 0.2037 | train_acc: 0.9391 | test_loss: 0.2643 | test_acc: 0.9315\n",
      "Epoch: 255 | train_loss: 0.1964 | train_acc: 0.9491 | test_loss: 0.2226 | test_acc: 0.9257\n",
      "Epoch: 256 | train_loss: 0.1944 | train_acc: 0.9348 | test_loss: 0.2534 | test_acc: 0.9264\n",
      "Epoch: 257 | train_loss: 0.1805 | train_acc: 0.9398 | test_loss: 0.2605 | test_acc: 0.9225\n",
      "Epoch: 258 | train_loss: 0.1824 | train_acc: 0.9414 | test_loss: 0.1880 | test_acc: 0.9302\n",
      "Epoch: 259 | train_loss: 0.2159 | train_acc: 0.9356 | test_loss: 0.1916 | test_acc: 0.9225\n",
      "Epoch: 260 | train_loss: 0.2131 | train_acc: 0.9335 | test_loss: 0.2082 | test_acc: 0.9328\n",
      "Epoch: 261 | train_loss: 0.2078 | train_acc: 0.9302 | test_loss: 0.3015 | test_acc: 0.9283\n",
      "Epoch: 262 | train_loss: 0.1873 | train_acc: 0.9393 | test_loss: 0.2519 | test_acc: 0.9302\n",
      "Epoch: 263 | train_loss: 0.1848 | train_acc: 0.9430 | test_loss: 0.2722 | test_acc: 0.9296\n",
      "Epoch: 264 | train_loss: 0.1940 | train_acc: 0.9398 | test_loss: 0.2874 | test_acc: 0.9167\n",
      "Epoch: 265 | train_loss: 0.2167 | train_acc: 0.9325 | test_loss: 0.2142 | test_acc: 0.9302\n",
      "Epoch: 266 | train_loss: 0.2021 | train_acc: 0.9423 | test_loss: 0.2315 | test_acc: 0.9276\n",
      "Epoch: 267 | train_loss: 0.2297 | train_acc: 0.9323 | test_loss: 0.2271 | test_acc: 0.9147\n",
      "Epoch: 268 | train_loss: 0.2319 | train_acc: 0.9265 | test_loss: 0.3103 | test_acc: 0.9335\n",
      "Epoch: 269 | train_loss: 0.2231 | train_acc: 0.9317 | test_loss: 0.3750 | test_acc: 0.8850\n",
      "Epoch: 270 | train_loss: 0.2043 | train_acc: 0.9372 | test_loss: 0.2430 | test_acc: 0.9289\n",
      "Epoch: 271 | train_loss: 0.2159 | train_acc: 0.9401 | test_loss: 0.2214 | test_acc: 0.9373\n",
      "Epoch: 272 | train_loss: 0.2166 | train_acc: 0.9297 | test_loss: 0.2801 | test_acc: 0.9212\n",
      "Epoch: 273 | train_loss: 0.2154 | train_acc: 0.9335 | test_loss: 0.2359 | test_acc: 0.9464\n",
      "Epoch: 274 | train_loss: 0.2028 | train_acc: 0.9415 | test_loss: 0.2163 | test_acc: 0.9244\n",
      "Epoch: 275 | train_loss: 0.2132 | train_acc: 0.9393 | test_loss: 0.1813 | test_acc: 0.9315\n",
      "Epoch: 276 | train_loss: 0.2341 | train_acc: 0.9280 | test_loss: 0.2366 | test_acc: 0.9173\n",
      "Epoch: 277 | train_loss: 0.2232 | train_acc: 0.9344 | test_loss: 0.2682 | test_acc: 0.9083\n",
      "Epoch: 278 | train_loss: 0.2046 | train_acc: 0.9356 | test_loss: 0.2010 | test_acc: 0.9354\n",
      "Epoch: 279 | train_loss: 0.1974 | train_acc: 0.9360 | test_loss: 0.4046 | test_acc: 0.9244\n",
      "Epoch: 280 | train_loss: 0.2128 | train_acc: 0.9339 | test_loss: 0.2243 | test_acc: 0.9302\n",
      "Epoch: 281 | train_loss: 0.1941 | train_acc: 0.9407 | test_loss: 0.2186 | test_acc: 0.9289\n",
      "Epoch: 282 | train_loss: 0.2299 | train_acc: 0.9398 | test_loss: 0.3912 | test_acc: 0.9264\n",
      "Epoch: 283 | train_loss: 0.2213 | train_acc: 0.9304 | test_loss: 0.3010 | test_acc: 0.9231\n",
      "Epoch: 284 | train_loss: 0.2056 | train_acc: 0.9369 | test_loss: 0.2718 | test_acc: 0.9289\n",
      "Epoch: 285 | train_loss: 0.1963 | train_acc: 0.9407 | test_loss: 0.2465 | test_acc: 0.9212\n",
      "Epoch: 286 | train_loss: 0.1931 | train_acc: 0.9411 | test_loss: 0.3201 | test_acc: 0.9167\n",
      "Epoch: 287 | train_loss: 0.2100 | train_acc: 0.9336 | test_loss: 0.2186 | test_acc: 0.9380\n",
      "Epoch: 288 | train_loss: 0.1883 | train_acc: 0.9370 | test_loss: 0.1665 | test_acc: 0.9451\n",
      "Epoch: 289 | train_loss: 0.1954 | train_acc: 0.9377 | test_loss: 0.2098 | test_acc: 0.9335\n",
      "Epoch: 290 | train_loss: 0.1986 | train_acc: 0.9415 | test_loss: 0.1976 | test_acc: 0.9322\n",
      "Epoch: 291 | train_loss: 0.1737 | train_acc: 0.9422 | test_loss: 0.1940 | test_acc: 0.9380\n",
      "Epoch: 292 | train_loss: 0.1706 | train_acc: 0.9451 | test_loss: 0.2266 | test_acc: 0.9373\n",
      "Epoch: 293 | train_loss: 0.1611 | train_acc: 0.9493 | test_loss: 0.3076 | test_acc: 0.9296\n",
      "Epoch: 294 | train_loss: 0.1890 | train_acc: 0.9462 | test_loss: 0.2503 | test_acc: 0.9289\n",
      "Epoch: 295 | train_loss: 0.1901 | train_acc: 0.9417 | test_loss: 0.1841 | test_acc: 0.9309\n",
      "Epoch: 296 | train_loss: 0.2057 | train_acc: 0.9367 | test_loss: 0.2047 | test_acc: 0.9373\n",
      "Epoch: 297 | train_loss: 0.1940 | train_acc: 0.9396 | test_loss: 0.2181 | test_acc: 0.9354\n",
      "Epoch: 298 | train_loss: 0.2047 | train_acc: 0.9369 | test_loss: 0.2268 | test_acc: 0.9193\n",
      "Epoch: 299 | train_loss: 0.2024 | train_acc: 0.9373 | test_loss: 0.2815 | test_acc: 0.9244\n",
      "Epoch: 300 | train_loss: 0.1805 | train_acc: 0.9462 | test_loss: 0.1748 | test_acc: 0.9406\n",
      "Epoch: 301 | train_loss: 0.1775 | train_acc: 0.9428 | test_loss: 0.2468 | test_acc: 0.9386\n",
      "Epoch: 302 | train_loss: 0.1700 | train_acc: 0.9441 | test_loss: 0.2212 | test_acc: 0.9322\n",
      "Epoch: 303 | train_loss: 0.1717 | train_acc: 0.9422 | test_loss: 0.1799 | test_acc: 0.9289\n",
      "Epoch: 304 | train_loss: 0.1722 | train_acc: 0.9436 | test_loss: 0.1755 | test_acc: 0.9315\n",
      "Epoch: 305 | train_loss: 0.1978 | train_acc: 0.9506 | test_loss: 0.1649 | test_acc: 0.9406\n",
      "Epoch: 306 | train_loss: 0.2059 | train_acc: 0.9381 | test_loss: 0.2047 | test_acc: 0.9264\n",
      "Epoch: 307 | train_loss: 0.2064 | train_acc: 0.9356 | test_loss: 0.2678 | test_acc: 0.9128\n",
      "Epoch: 308 | train_loss: 0.1864 | train_acc: 0.9415 | test_loss: 0.2351 | test_acc: 0.9283\n",
      "Epoch: 309 | train_loss: 0.1833 | train_acc: 0.9412 | test_loss: 0.1609 | test_acc: 0.9406\n",
      "Epoch: 310 | train_loss: 0.1843 | train_acc: 0.9402 | test_loss: 0.1610 | test_acc: 0.9380\n",
      "Epoch: 311 | train_loss: 0.1957 | train_acc: 0.9412 | test_loss: 0.7136 | test_acc: 0.9367\n",
      "Epoch: 312 | train_loss: 0.1842 | train_acc: 0.9388 | test_loss: 0.2074 | test_acc: 0.9212\n",
      "Epoch: 313 | train_loss: 0.1705 | train_acc: 0.9474 | test_loss: 0.2055 | test_acc: 0.9354\n",
      "Epoch: 314 | train_loss: 0.1622 | train_acc: 0.9503 | test_loss: 0.2596 | test_acc: 0.9328\n",
      "Epoch: 315 | train_loss: 0.1984 | train_acc: 0.9394 | test_loss: 0.2946 | test_acc: 0.9302\n",
      "Epoch: 316 | train_loss: 0.1800 | train_acc: 0.9414 | test_loss: 0.2847 | test_acc: 0.9309\n",
      "Epoch: 317 | train_loss: 0.1747 | train_acc: 0.9472 | test_loss: 0.1573 | test_acc: 0.9328\n",
      "Epoch: 318 | train_loss: 0.1748 | train_acc: 0.9430 | test_loss: 0.1655 | test_acc: 0.9354\n",
      "Epoch: 319 | train_loss: 0.1705 | train_acc: 0.9516 | test_loss: 0.3935 | test_acc: 0.9348\n",
      "Epoch: 320 | train_loss: 0.1693 | train_acc: 0.9475 | test_loss: 0.1861 | test_acc: 0.9335\n",
      "Epoch: 321 | train_loss: 0.1628 | train_acc: 0.9462 | test_loss: 0.3108 | test_acc: 0.9335\n",
      "Epoch: 322 | train_loss: 0.1970 | train_acc: 0.9396 | test_loss: 0.2384 | test_acc: 0.9276\n",
      "Epoch: 323 | train_loss: 0.1781 | train_acc: 0.9425 | test_loss: 0.4527 | test_acc: 0.9264\n",
      "Epoch: 324 | train_loss: 0.1699 | train_acc: 0.9415 | test_loss: 0.1600 | test_acc: 0.9432\n",
      "Epoch: 325 | train_loss: 0.1687 | train_acc: 0.9503 | test_loss: 0.2791 | test_acc: 0.9335\n",
      "Epoch: 326 | train_loss: 0.1896 | train_acc: 0.9459 | test_loss: 0.2703 | test_acc: 0.9348\n",
      "Epoch: 327 | train_loss: 0.1932 | train_acc: 0.9420 | test_loss: 0.1543 | test_acc: 0.9406\n",
      "Epoch: 328 | train_loss: 0.1989 | train_acc: 0.9412 | test_loss: 0.2269 | test_acc: 0.9244\n",
      "Epoch: 329 | train_loss: 0.1888 | train_acc: 0.9383 | test_loss: 0.2296 | test_acc: 0.9134\n",
      "Epoch: 330 | train_loss: 0.1783 | train_acc: 0.9422 | test_loss: 0.1684 | test_acc: 0.9373\n",
      "Epoch: 331 | train_loss: 0.1831 | train_acc: 0.9411 | test_loss: 0.2605 | test_acc: 0.9257\n",
      "Epoch: 332 | train_loss: 0.1841 | train_acc: 0.9438 | test_loss: 0.2314 | test_acc: 0.9264\n",
      "Epoch: 333 | train_loss: 0.1784 | train_acc: 0.9470 | test_loss: 0.2044 | test_acc: 0.9393\n",
      "Epoch: 334 | train_loss: 0.1845 | train_acc: 0.9475 | test_loss: 0.2516 | test_acc: 0.9335\n",
      "Epoch: 335 | train_loss: 0.1775 | train_acc: 0.9433 | test_loss: 0.4258 | test_acc: 0.9238\n",
      "Epoch: 336 | train_loss: 0.1756 | train_acc: 0.9432 | test_loss: 0.2149 | test_acc: 0.9412\n",
      "Epoch: 337 | train_loss: 0.1864 | train_acc: 0.9449 | test_loss: 0.2546 | test_acc: 0.9322\n",
      "Epoch: 338 | train_loss: 0.1712 | train_acc: 0.9435 | test_loss: 0.1774 | test_acc: 0.9380\n",
      "Epoch: 339 | train_loss: 0.1671 | train_acc: 0.9462 | test_loss: 0.1965 | test_acc: 0.9264\n",
      "Epoch: 340 | train_loss: 0.1676 | train_acc: 0.9480 | test_loss: 0.3800 | test_acc: 0.9432\n",
      "Epoch: 341 | train_loss: 0.1775 | train_acc: 0.9467 | test_loss: 0.2357 | test_acc: 0.9360\n",
      "Epoch: 342 | train_loss: 0.1838 | train_acc: 0.9457 | test_loss: 0.2320 | test_acc: 0.9367\n",
      "Epoch: 343 | train_loss: 0.1810 | train_acc: 0.9427 | test_loss: 0.1788 | test_acc: 0.9360\n",
      "Epoch: 344 | train_loss: 0.1723 | train_acc: 0.9474 | test_loss: 0.1886 | test_acc: 0.9354\n",
      "Epoch: 345 | train_loss: 0.1736 | train_acc: 0.9480 | test_loss: 0.1702 | test_acc: 0.9406\n",
      "Epoch: 346 | train_loss: 0.1863 | train_acc: 0.9475 | test_loss: 0.1828 | test_acc: 0.9322\n",
      "Epoch: 347 | train_loss: 0.1856 | train_acc: 0.9440 | test_loss: 0.2238 | test_acc: 0.9341\n",
      "Epoch: 348 | train_loss: 0.1740 | train_acc: 0.9414 | test_loss: 0.2016 | test_acc: 0.9231\n",
      "Epoch: 349 | train_loss: 0.1676 | train_acc: 0.9451 | test_loss: 0.3769 | test_acc: 0.9399\n",
      "Epoch: 350 | train_loss: 0.1820 | train_acc: 0.9469 | test_loss: 0.1971 | test_acc: 0.9360\n",
      "Epoch: 351 | train_loss: 0.1667 | train_acc: 0.9496 | test_loss: 0.2668 | test_acc: 0.9328\n",
      "Epoch: 352 | train_loss: 0.1793 | train_acc: 0.9465 | test_loss: 0.1592 | test_acc: 0.9360\n",
      "Epoch: 353 | train_loss: 0.1749 | train_acc: 0.9436 | test_loss: 0.3043 | test_acc: 0.9296\n",
      "Epoch: 354 | train_loss: 0.1756 | train_acc: 0.9444 | test_loss: 0.3420 | test_acc: 0.9264\n",
      "Epoch: 355 | train_loss: 0.2063 | train_acc: 0.9391 | test_loss: 0.2169 | test_acc: 0.9238\n",
      "Epoch: 356 | train_loss: 0.1825 | train_acc: 0.9415 | test_loss: 0.2568 | test_acc: 0.9393\n",
      "Epoch: 357 | train_loss: 0.1952 | train_acc: 0.9454 | test_loss: 0.2921 | test_acc: 0.9386\n",
      "Epoch: 358 | train_loss: 0.2032 | train_acc: 0.9335 | test_loss: 0.1926 | test_acc: 0.9283\n",
      "Epoch: 359 | train_loss: 0.1831 | train_acc: 0.9486 | test_loss: 0.3078 | test_acc: 0.8921\n",
      "Epoch: 360 | train_loss: 0.1887 | train_acc: 0.9425 | test_loss: 0.2807 | test_acc: 0.9432\n",
      "Epoch: 361 | train_loss: 0.1686 | train_acc: 0.9464 | test_loss: 0.1766 | test_acc: 0.9348\n",
      "Epoch: 362 | train_loss: 0.1837 | train_acc: 0.9456 | test_loss: 0.2842 | test_acc: 0.9354\n",
      "Epoch: 363 | train_loss: 0.1843 | train_acc: 0.9461 | test_loss: 0.1809 | test_acc: 0.9341\n",
      "Epoch: 364 | train_loss: 0.1715 | train_acc: 0.9449 | test_loss: 0.2009 | test_acc: 0.9309\n",
      "Epoch: 365 | train_loss: 0.1699 | train_acc: 0.9440 | test_loss: 0.2540 | test_acc: 0.9348\n",
      "Epoch: 366 | train_loss: 0.1774 | train_acc: 0.9456 | test_loss: 0.2707 | test_acc: 0.9173\n",
      "Epoch: 367 | train_loss: 0.1862 | train_acc: 0.9459 | test_loss: 0.2160 | test_acc: 0.9302\n",
      "Epoch: 368 | train_loss: 0.1708 | train_acc: 0.9451 | test_loss: 0.2142 | test_acc: 0.9367\n",
      "Epoch: 369 | train_loss: 0.1782 | train_acc: 0.9412 | test_loss: 0.3682 | test_acc: 0.9283\n",
      "Epoch: 370 | train_loss: 0.1989 | train_acc: 0.9388 | test_loss: 0.2026 | test_acc: 0.9231\n",
      "Epoch: 371 | train_loss: 0.1853 | train_acc: 0.9436 | test_loss: 0.2238 | test_acc: 0.9205\n",
      "Epoch: 372 | train_loss: 0.1874 | train_acc: 0.9420 | test_loss: 0.2199 | test_acc: 0.9354\n",
      "Epoch: 373 | train_loss: 0.1883 | train_acc: 0.9454 | test_loss: 0.1538 | test_acc: 0.9406\n",
      "Epoch: 374 | train_loss: 0.2089 | train_acc: 0.9399 | test_loss: 0.2958 | test_acc: 0.9225\n",
      "Epoch: 375 | train_loss: 0.2510 | train_acc: 0.9325 | test_loss: 0.2530 | test_acc: 0.9257\n",
      "Epoch: 376 | train_loss: 0.2271 | train_acc: 0.9297 | test_loss: 0.1988 | test_acc: 0.9302\n",
      "Epoch: 377 | train_loss: 0.2046 | train_acc: 0.9369 | test_loss: 0.1879 | test_acc: 0.9380\n",
      "Epoch: 378 | train_loss: 0.1932 | train_acc: 0.9380 | test_loss: 0.2546 | test_acc: 0.9238\n",
      "Epoch: 379 | train_loss: 0.1981 | train_acc: 0.9390 | test_loss: 0.2813 | test_acc: 0.9296\n",
      "Epoch: 380 | train_loss: 0.2088 | train_acc: 0.9352 | test_loss: 0.2347 | test_acc: 0.9386\n",
      "Epoch: 381 | train_loss: 0.2151 | train_acc: 0.9328 | test_loss: 0.2023 | test_acc: 0.9341\n",
      "Epoch: 382 | train_loss: 0.1706 | train_acc: 0.9438 | test_loss: 0.2167 | test_acc: 0.9386\n",
      "Epoch: 383 | train_loss: 0.1755 | train_acc: 0.9469 | test_loss: 0.2457 | test_acc: 0.9322\n",
      "Epoch: 384 | train_loss: 0.1550 | train_acc: 0.9485 | test_loss: 0.2281 | test_acc: 0.9289\n",
      "Epoch: 385 | train_loss: 0.1752 | train_acc: 0.9475 | test_loss: 0.2173 | test_acc: 0.9328\n",
      "Epoch: 386 | train_loss: 0.1853 | train_acc: 0.9461 | test_loss: 0.2490 | test_acc: 0.9283\n",
      "Epoch: 387 | train_loss: 0.2152 | train_acc: 0.9451 | test_loss: 0.1781 | test_acc: 0.9373\n",
      "Epoch: 388 | train_loss: 0.2014 | train_acc: 0.9381 | test_loss: 0.1793 | test_acc: 0.9283\n",
      "Epoch: 389 | train_loss: 0.1818 | train_acc: 0.9444 | test_loss: 0.2172 | test_acc: 0.9270\n",
      "Epoch: 390 | train_loss: 0.1605 | train_acc: 0.9501 | test_loss: 0.2291 | test_acc: 0.9399\n",
      "Epoch: 391 | train_loss: 0.1513 | train_acc: 0.9546 | test_loss: 0.2182 | test_acc: 0.9380\n",
      "Epoch: 392 | train_loss: 0.1632 | train_acc: 0.9499 | test_loss: 0.2883 | test_acc: 0.9315\n",
      "Epoch: 393 | train_loss: 0.2091 | train_acc: 0.9461 | test_loss: 0.1822 | test_acc: 0.9315\n",
      "Epoch: 394 | train_loss: 0.2352 | train_acc: 0.9254 | test_loss: 0.2178 | test_acc: 0.9367\n",
      "Epoch: 395 | train_loss: 0.1982 | train_acc: 0.9386 | test_loss: 0.2005 | test_acc: 0.9283\n",
      "Epoch: 396 | train_loss: 0.1993 | train_acc: 0.9398 | test_loss: 0.1980 | test_acc: 0.9360\n",
      "Epoch: 397 | train_loss: 0.2005 | train_acc: 0.9377 | test_loss: 0.1882 | test_acc: 0.9373\n",
      "Epoch: 398 | train_loss: 0.2041 | train_acc: 0.9381 | test_loss: 0.2322 | test_acc: 0.9173\n",
      "Epoch: 399 | train_loss: 0.1756 | train_acc: 0.9453 | test_loss: 0.2508 | test_acc: 0.9380\n",
      "Epoch: 400 | train_loss: 0.1794 | train_acc: 0.9423 | test_loss: 0.2189 | test_acc: 0.9393\n",
      "Epoch: 401 | train_loss: 0.2029 | train_acc: 0.9485 | test_loss: 0.2395 | test_acc: 0.9244\n",
      "Epoch: 402 | train_loss: 0.1951 | train_acc: 0.9420 | test_loss: 0.1946 | test_acc: 0.9302\n",
      "Epoch: 403 | train_loss: 0.1903 | train_acc: 0.9406 | test_loss: 0.2627 | test_acc: 0.9289\n",
      "Epoch: 404 | train_loss: 0.1799 | train_acc: 0.9415 | test_loss: 0.1689 | test_acc: 0.9432\n",
      "Epoch: 405 | train_loss: 0.1525 | train_acc: 0.9499 | test_loss: 0.1696 | test_acc: 0.9425\n",
      "Epoch: 406 | train_loss: 0.1775 | train_acc: 0.9453 | test_loss: 0.2687 | test_acc: 0.9322\n",
      "Epoch: 407 | train_loss: 0.1607 | train_acc: 0.9477 | test_loss: 0.1659 | test_acc: 0.9419\n",
      "Epoch: 408 | train_loss: 0.1673 | train_acc: 0.9470 | test_loss: 0.2943 | test_acc: 0.9335\n",
      "Epoch: 409 | train_loss: 0.1596 | train_acc: 0.9512 | test_loss: 0.1940 | test_acc: 0.9393\n",
      "Epoch: 410 | train_loss: 0.1627 | train_acc: 0.9478 | test_loss: 0.2539 | test_acc: 0.9289\n",
      "Epoch: 411 | train_loss: 0.1632 | train_acc: 0.9511 | test_loss: 0.1832 | test_acc: 0.9386\n",
      "Epoch: 412 | train_loss: 0.1766 | train_acc: 0.9475 | test_loss: 0.2228 | test_acc: 0.9354\n",
      "Epoch: 413 | train_loss: 0.1910 | train_acc: 0.9422 | test_loss: 0.2105 | test_acc: 0.9309\n",
      "Epoch: 414 | train_loss: 0.1909 | train_acc: 0.9443 | test_loss: 0.2400 | test_acc: 0.9386\n",
      "Epoch: 415 | train_loss: 0.1824 | train_acc: 0.9478 | test_loss: 0.1952 | test_acc: 0.9341\n",
      "Epoch: 416 | train_loss: 0.1681 | train_acc: 0.9440 | test_loss: 0.2113 | test_acc: 0.9360\n",
      "Epoch: 417 | train_loss: 0.1556 | train_acc: 0.9501 | test_loss: 0.2471 | test_acc: 0.9328\n",
      "Epoch: 418 | train_loss: 0.1658 | train_acc: 0.9453 | test_loss: 0.3700 | test_acc: 0.9025\n",
      "Epoch: 419 | train_loss: 0.1672 | train_acc: 0.9438 | test_loss: 0.2077 | test_acc: 0.9328\n",
      "Epoch: 420 | train_loss: 0.1513 | train_acc: 0.9514 | test_loss: 0.3091 | test_acc: 0.9302\n",
      "Epoch: 421 | train_loss: 0.1682 | train_acc: 0.9456 | test_loss: 0.2938 | test_acc: 0.9380\n",
      "Epoch: 422 | train_loss: 0.1805 | train_acc: 0.9477 | test_loss: 0.2439 | test_acc: 0.9283\n",
      "Epoch: 423 | train_loss: 0.1877 | train_acc: 0.9448 | test_loss: 0.1825 | test_acc: 0.9367\n",
      "Epoch: 424 | train_loss: 0.1889 | train_acc: 0.9380 | test_loss: 0.2833 | test_acc: 0.9360\n",
      "Epoch: 425 | train_loss: 0.1836 | train_acc: 0.9446 | test_loss: 0.3995 | test_acc: 0.9276\n",
      "Epoch: 426 | train_loss: 0.1913 | train_acc: 0.9394 | test_loss: 0.1873 | test_acc: 0.9464\n",
      "Epoch: 427 | train_loss: 0.1730 | train_acc: 0.9493 | test_loss: 0.2047 | test_acc: 0.9296\n",
      "Epoch: 428 | train_loss: 0.1630 | train_acc: 0.9457 | test_loss: 0.1969 | test_acc: 0.9309\n",
      "Epoch: 429 | train_loss: 0.1633 | train_acc: 0.9485 | test_loss: 0.3175 | test_acc: 0.9335\n",
      "Epoch: 430 | train_loss: 0.1766 | train_acc: 0.9485 | test_loss: 0.2066 | test_acc: 0.9180\n",
      "Epoch: 431 | train_loss: 0.2008 | train_acc: 0.9419 | test_loss: 0.4206 | test_acc: 0.9063\n",
      "Epoch: 432 | train_loss: 0.2007 | train_acc: 0.9414 | test_loss: 0.2241 | test_acc: 0.9341\n",
      "Epoch: 433 | train_loss: 0.1818 | train_acc: 0.9381 | test_loss: 0.2462 | test_acc: 0.9193\n",
      "Epoch: 434 | train_loss: 0.1978 | train_acc: 0.9464 | test_loss: 0.2631 | test_acc: 0.9367\n",
      "Epoch: 435 | train_loss: 0.2148 | train_acc: 0.9307 | test_loss: 0.1973 | test_acc: 0.9186\n",
      "Epoch: 436 | train_loss: 0.1820 | train_acc: 0.9394 | test_loss: 0.1996 | test_acc: 0.9406\n",
      "Epoch: 437 | train_loss: 0.1898 | train_acc: 0.9453 | test_loss: 0.3282 | test_acc: 0.9167\n",
      "Epoch: 438 | train_loss: 0.1861 | train_acc: 0.9401 | test_loss: 0.1983 | test_acc: 0.9322\n",
      "Epoch: 439 | train_loss: 0.2004 | train_acc: 0.9406 | test_loss: 0.3372 | test_acc: 0.9386\n",
      "Epoch: 440 | train_loss: 0.2033 | train_acc: 0.9390 | test_loss: 0.2302 | test_acc: 0.9128\n",
      "Epoch: 441 | train_loss: 0.1859 | train_acc: 0.9459 | test_loss: 0.1845 | test_acc: 0.9367\n",
      "Epoch: 442 | train_loss: 0.1591 | train_acc: 0.9461 | test_loss: 0.2228 | test_acc: 0.9490\n",
      "Epoch: 443 | train_loss: 0.1649 | train_acc: 0.9517 | test_loss: 0.2577 | test_acc: 0.9367\n",
      "Epoch: 444 | train_loss: 0.1623 | train_acc: 0.9482 | test_loss: 0.6860 | test_acc: 0.9360\n",
      "Epoch: 445 | train_loss: 0.1697 | train_acc: 0.9490 | test_loss: 0.1856 | test_acc: 0.9393\n",
      "Epoch: 446 | train_loss: 0.1708 | train_acc: 0.9467 | test_loss: 0.1838 | test_acc: 0.9425\n",
      "Epoch: 447 | train_loss: 0.2014 | train_acc: 0.9367 | test_loss: 0.1917 | test_acc: 0.9283\n",
      "Epoch: 448 | train_loss: 0.1776 | train_acc: 0.9456 | test_loss: 0.2267 | test_acc: 0.9451\n",
      "Epoch: 449 | train_loss: 0.1976 | train_acc: 0.9441 | test_loss: 0.2010 | test_acc: 0.9238\n",
      "Epoch: 450 | train_loss: 0.2076 | train_acc: 0.9348 | test_loss: 0.2376 | test_acc: 0.9257\n",
      "Epoch: 451 | train_loss: 0.2020 | train_acc: 0.9409 | test_loss: 0.1957 | test_acc: 0.9470\n",
      "Epoch: 452 | train_loss: 0.1824 | train_acc: 0.9420 | test_loss: 0.2471 | test_acc: 0.9205\n",
      "Epoch: 453 | train_loss: 0.1755 | train_acc: 0.9438 | test_loss: 0.2746 | test_acc: 0.9102\n",
      "Epoch: 454 | train_loss: 0.1800 | train_acc: 0.9449 | test_loss: 0.1975 | test_acc: 0.9438\n",
      "Epoch: 455 | train_loss: 0.1938 | train_acc: 0.9411 | test_loss: 0.1824 | test_acc: 0.9348\n",
      "Epoch: 456 | train_loss: 0.1849 | train_acc: 0.9383 | test_loss: 0.3343 | test_acc: 0.9115\n",
      "Epoch: 457 | train_loss: 0.1792 | train_acc: 0.9436 | test_loss: 0.1974 | test_acc: 0.9302\n",
      "Epoch: 458 | train_loss: 0.1731 | train_acc: 0.9491 | test_loss: 0.3045 | test_acc: 0.9341\n",
      "Epoch: 459 | train_loss: 0.1634 | train_acc: 0.9507 | test_loss: 0.1939 | test_acc: 0.9341\n",
      "Epoch: 460 | train_loss: 0.1499 | train_acc: 0.9516 | test_loss: 0.1748 | test_acc: 0.9406\n",
      "Epoch: 461 | train_loss: 0.1523 | train_acc: 0.9517 | test_loss: 0.1864 | test_acc: 0.9360\n",
      "Epoch: 462 | train_loss: 0.1571 | train_acc: 0.9514 | test_loss: 0.1890 | test_acc: 0.9360\n",
      "Epoch: 463 | train_loss: 0.1643 | train_acc: 0.9474 | test_loss: 0.2593 | test_acc: 0.9360\n",
      "Epoch: 464 | train_loss: 0.1745 | train_acc: 0.9475 | test_loss: 0.2208 | test_acc: 0.9386\n",
      "Epoch: 465 | train_loss: 0.1661 | train_acc: 0.9504 | test_loss: 0.2674 | test_acc: 0.9244\n",
      "Epoch: 466 | train_loss: 0.1534 | train_acc: 0.9486 | test_loss: 0.1581 | test_acc: 0.9451\n",
      "Epoch: 467 | train_loss: 0.1582 | train_acc: 0.9520 | test_loss: 0.3154 | test_acc: 0.9399\n",
      "Epoch: 468 | train_loss: 0.1610 | train_acc: 0.9488 | test_loss: 0.1714 | test_acc: 0.9432\n",
      "Epoch: 469 | train_loss: 0.1795 | train_acc: 0.9456 | test_loss: 0.2405 | test_acc: 0.9296\n",
      "Epoch: 470 | train_loss: 0.1826 | train_acc: 0.9435 | test_loss: 0.1791 | test_acc: 0.9322\n",
      "Epoch: 471 | train_loss: 0.1892 | train_acc: 0.9440 | test_loss: 0.2412 | test_acc: 0.9296\n",
      "Epoch: 472 | train_loss: 0.1761 | train_acc: 0.9498 | test_loss: 0.1687 | test_acc: 0.9386\n",
      "Epoch: 473 | train_loss: 0.1877 | train_acc: 0.9401 | test_loss: 0.2618 | test_acc: 0.9348\n",
      "Epoch: 474 | train_loss: 0.1717 | train_acc: 0.9477 | test_loss: 0.3947 | test_acc: 0.9167\n",
      "Epoch: 475 | train_loss: 0.1749 | train_acc: 0.9462 | test_loss: 0.1996 | test_acc: 0.9315\n",
      "Epoch: 476 | train_loss: 0.1677 | train_acc: 0.9453 | test_loss: 0.3004 | test_acc: 0.9399\n",
      "Epoch: 477 | train_loss: 0.1428 | train_acc: 0.9548 | test_loss: 0.1604 | test_acc: 0.9477\n",
      "Epoch: 478 | train_loss: 0.1485 | train_acc: 0.9543 | test_loss: 0.1976 | test_acc: 0.9399\n",
      "Epoch: 479 | train_loss: 0.1688 | train_acc: 0.9477 | test_loss: 0.2213 | test_acc: 0.9412\n",
      "Epoch: 480 | train_loss: 0.1837 | train_acc: 0.9517 | test_loss: 0.2892 | test_acc: 0.9076\n",
      "Epoch: 481 | train_loss: 0.2081 | train_acc: 0.9338 | test_loss: 0.2451 | test_acc: 0.9309\n",
      "Epoch: 482 | train_loss: 0.2115 | train_acc: 0.9402 | test_loss: 0.3265 | test_acc: 0.9483\n",
      "Epoch: 483 | train_loss: 0.1805 | train_acc: 0.9436 | test_loss: 0.1745 | test_acc: 0.9322\n",
      "Epoch: 484 | train_loss: 0.1941 | train_acc: 0.9428 | test_loss: 0.1566 | test_acc: 0.9335\n",
      "Epoch: 485 | train_loss: 0.1977 | train_acc: 0.9449 | test_loss: 0.2502 | test_acc: 0.9212\n",
      "Epoch: 486 | train_loss: 0.1829 | train_acc: 0.9478 | test_loss: 0.4238 | test_acc: 0.9335\n",
      "Epoch: 487 | train_loss: 0.1742 | train_acc: 0.9444 | test_loss: 0.2506 | test_acc: 0.9276\n",
      "Epoch: 488 | train_loss: 0.1707 | train_acc: 0.9470 | test_loss: 0.2283 | test_acc: 0.9367\n",
      "Epoch: 489 | train_loss: 0.1829 | train_acc: 0.9485 | test_loss: 0.1985 | test_acc: 0.9419\n",
      "Epoch: 490 | train_loss: 0.1795 | train_acc: 0.9441 | test_loss: 0.1742 | test_acc: 0.9393\n",
      "Epoch: 491 | train_loss: 0.1685 | train_acc: 0.9440 | test_loss: 0.1929 | test_acc: 0.9367\n",
      "Epoch: 492 | train_loss: 0.1584 | train_acc: 0.9525 | test_loss: 0.1802 | test_acc: 0.9425\n",
      "Epoch: 493 | train_loss: 0.1895 | train_acc: 0.9482 | test_loss: 0.2177 | test_acc: 0.9425\n",
      "Epoch: 494 | train_loss: 0.1616 | train_acc: 0.9472 | test_loss: 0.2237 | test_acc: 0.9393\n",
      "Epoch: 495 | train_loss: 0.1595 | train_acc: 0.9496 | test_loss: 0.2040 | test_acc: 0.9283\n",
      "Epoch: 496 | train_loss: 0.1627 | train_acc: 0.9467 | test_loss: 0.1843 | test_acc: 0.9276\n",
      "Epoch: 497 | train_loss: 0.1773 | train_acc: 0.9446 | test_loss: 0.4380 | test_acc: 0.9348\n",
      "Epoch: 498 | train_loss: 0.1529 | train_acc: 0.9509 | test_loss: 0.2734 | test_acc: 0.9218\n",
      "Epoch: 499 | train_loss: 0.1557 | train_acc: 0.9509 | test_loss: 0.2575 | test_acc: 0.9186\n",
      "Epoch: 500 | train_loss: 0.1603 | train_acc: 0.9527 | test_loss: 0.1787 | test_acc: 0.9335\n",
      "Epoch: 501 | train_loss: 0.1494 | train_acc: 0.9517 | test_loss: 0.3221 | test_acc: 0.9315\n",
      "Epoch: 502 | train_loss: 0.1595 | train_acc: 0.9461 | test_loss: 0.1729 | test_acc: 0.9406\n",
      "Epoch: 503 | train_loss: 0.1624 | train_acc: 0.9516 | test_loss: 0.2863 | test_acc: 0.9386\n",
      "Epoch: 504 | train_loss: 0.1547 | train_acc: 0.9540 | test_loss: 0.1639 | test_acc: 0.9386\n",
      "Epoch: 505 | train_loss: 0.1842 | train_acc: 0.9456 | test_loss: 0.1833 | test_acc: 0.9425\n",
      "Epoch: 506 | train_loss: 0.1660 | train_acc: 0.9467 | test_loss: 0.2241 | test_acc: 0.9348\n",
      "Epoch: 507 | train_loss: 0.1666 | train_acc: 0.9456 | test_loss: 0.2394 | test_acc: 0.9406\n",
      "Epoch: 508 | train_loss: 0.1563 | train_acc: 0.9499 | test_loss: 0.3497 | test_acc: 0.9367\n",
      "Epoch: 509 | train_loss: 0.1641 | train_acc: 0.9453 | test_loss: 0.2776 | test_acc: 0.9360\n",
      "Epoch: 510 | train_loss: 0.1882 | train_acc: 0.9464 | test_loss: 0.2510 | test_acc: 0.9147\n",
      "Epoch: 511 | train_loss: 0.1595 | train_acc: 0.9446 | test_loss: 0.2124 | test_acc: 0.9212\n",
      "Epoch: 512 | train_loss: 0.1717 | train_acc: 0.9467 | test_loss: 0.3985 | test_acc: 0.9438\n",
      "Epoch: 513 | train_loss: 0.1577 | train_acc: 0.9499 | test_loss: 0.3298 | test_acc: 0.9477\n",
      "Epoch: 514 | train_loss: 0.1612 | train_acc: 0.9546 | test_loss: 0.2444 | test_acc: 0.9406\n",
      "Epoch: 515 | train_loss: 0.1837 | train_acc: 0.9407 | test_loss: 0.1896 | test_acc: 0.9251\n",
      "Epoch: 516 | train_loss: 0.1640 | train_acc: 0.9474 | test_loss: 0.2425 | test_acc: 0.9335\n",
      "Epoch: 517 | train_loss: 0.1727 | train_acc: 0.9511 | test_loss: 0.1918 | test_acc: 0.9373\n",
      "Epoch: 518 | train_loss: 0.1733 | train_acc: 0.9477 | test_loss: 0.1905 | test_acc: 0.9270\n",
      "Epoch: 519 | train_loss: 0.1580 | train_acc: 0.9506 | test_loss: 0.2174 | test_acc: 0.9399\n",
      "Epoch: 520 | train_loss: 0.1846 | train_acc: 0.9406 | test_loss: 0.4016 | test_acc: 0.9257\n",
      "Epoch: 521 | train_loss: 0.1731 | train_acc: 0.9448 | test_loss: 0.2843 | test_acc: 0.9386\n",
      "Epoch: 522 | train_loss: 0.1610 | train_acc: 0.9490 | test_loss: 0.2019 | test_acc: 0.9419\n",
      "Epoch: 523 | train_loss: 0.1848 | train_acc: 0.9456 | test_loss: 0.2267 | test_acc: 0.9322\n",
      "Epoch: 524 | train_loss: 0.1934 | train_acc: 0.9490 | test_loss: 0.3849 | test_acc: 0.9225\n",
      "Epoch: 525 | train_loss: 0.2169 | train_acc: 0.9349 | test_loss: 0.3987 | test_acc: 0.9160\n",
      "Epoch: 526 | train_loss: 0.1964 | train_acc: 0.9449 | test_loss: 0.2045 | test_acc: 0.9367\n",
      "Epoch: 527 | train_loss: 0.1783 | train_acc: 0.9454 | test_loss: 0.1625 | test_acc: 0.9348\n",
      "Epoch: 528 | train_loss: 0.1736 | train_acc: 0.9459 | test_loss: 0.2549 | test_acc: 0.9380\n",
      "Epoch: 529 | train_loss: 0.1717 | train_acc: 0.9428 | test_loss: 0.1595 | test_acc: 0.9399\n",
      "Epoch: 530 | train_loss: 0.1612 | train_acc: 0.9470 | test_loss: 0.1620 | test_acc: 0.9444\n",
      "Epoch: 531 | train_loss: 0.1372 | train_acc: 0.9532 | test_loss: 0.2468 | test_acc: 0.9470\n",
      "Epoch: 532 | train_loss: 0.1403 | train_acc: 0.9546 | test_loss: 0.3022 | test_acc: 0.9354\n",
      "Epoch: 533 | train_loss: 0.1437 | train_acc: 0.9545 | test_loss: 0.1964 | test_acc: 0.9393\n",
      "Epoch: 534 | train_loss: 0.1409 | train_acc: 0.9530 | test_loss: 0.2095 | test_acc: 0.9354\n",
      "Epoch: 535 | train_loss: 0.1510 | train_acc: 0.9520 | test_loss: 0.2790 | test_acc: 0.9296\n",
      "Epoch: 536 | train_loss: 0.1532 | train_acc: 0.9524 | test_loss: 0.2071 | test_acc: 0.9360\n",
      "Epoch: 537 | train_loss: 0.1692 | train_acc: 0.9483 | test_loss: 0.2083 | test_acc: 0.9386\n",
      "Epoch: 538 | train_loss: 0.1535 | train_acc: 0.9527 | test_loss: 0.1912 | test_acc: 0.9432\n",
      "Epoch: 539 | train_loss: 0.1506 | train_acc: 0.9541 | test_loss: 0.1732 | test_acc: 0.9393\n",
      "Epoch: 540 | train_loss: 0.1477 | train_acc: 0.9532 | test_loss: 0.1900 | test_acc: 0.9393\n",
      "Epoch: 541 | train_loss: 0.1612 | train_acc: 0.9483 | test_loss: 0.2104 | test_acc: 0.9341\n",
      "Epoch: 542 | train_loss: 0.1621 | train_acc: 0.9462 | test_loss: 0.1654 | test_acc: 0.9406\n",
      "Epoch: 543 | train_loss: 0.1528 | train_acc: 0.9496 | test_loss: 0.1789 | test_acc: 0.9432\n",
      "Epoch: 544 | train_loss: 0.1344 | train_acc: 0.9595 | test_loss: 0.2318 | test_acc: 0.9328\n",
      "Epoch: 545 | train_loss: 0.1747 | train_acc: 0.9522 | test_loss: 0.1880 | test_acc: 0.9328\n",
      "Epoch: 546 | train_loss: 0.1829 | train_acc: 0.9474 | test_loss: 0.1831 | test_acc: 0.9296\n",
      "Epoch: 547 | train_loss: 0.2084 | train_acc: 0.9378 | test_loss: 0.1857 | test_acc: 0.9296\n",
      "Epoch: 548 | train_loss: 0.1729 | train_acc: 0.9456 | test_loss: 0.2791 | test_acc: 0.9341\n",
      "Epoch: 549 | train_loss: 0.1757 | train_acc: 0.9507 | test_loss: 0.2404 | test_acc: 0.9373\n",
      "Epoch: 550 | train_loss: 0.1676 | train_acc: 0.9507 | test_loss: 0.3072 | test_acc: 0.9399\n",
      "Epoch: 551 | train_loss: 0.1456 | train_acc: 0.9535 | test_loss: 0.2030 | test_acc: 0.9173\n",
      "Epoch: 552 | train_loss: 0.1534 | train_acc: 0.9546 | test_loss: 0.2387 | test_acc: 0.9419\n",
      "Epoch: 553 | train_loss: 0.1479 | train_acc: 0.9519 | test_loss: 0.1872 | test_acc: 0.9360\n",
      "Epoch: 554 | train_loss: 0.1371 | train_acc: 0.9564 | test_loss: 0.1537 | test_acc: 0.9438\n",
      "Epoch: 555 | train_loss: 0.1410 | train_acc: 0.9556 | test_loss: 0.1549 | test_acc: 0.9464\n",
      "Epoch: 556 | train_loss: 0.1659 | train_acc: 0.9541 | test_loss: 0.1601 | test_acc: 0.9373\n",
      "Epoch: 557 | train_loss: 0.1756 | train_acc: 0.9436 | test_loss: 0.2382 | test_acc: 0.9367\n",
      "Epoch: 558 | train_loss: 0.1547 | train_acc: 0.9519 | test_loss: 0.2986 | test_acc: 0.9341\n",
      "Epoch: 559 | train_loss: 0.1632 | train_acc: 0.9478 | test_loss: 0.1687 | test_acc: 0.9412\n",
      "Epoch: 560 | train_loss: 0.1494 | train_acc: 0.9491 | test_loss: 0.4563 | test_acc: 0.9419\n",
      "Epoch: 561 | train_loss: 0.1523 | train_acc: 0.9504 | test_loss: 0.2980 | test_acc: 0.9399\n",
      "Epoch: 562 | train_loss: 0.1665 | train_acc: 0.9551 | test_loss: 0.2187 | test_acc: 0.9464\n",
      "Epoch: 563 | train_loss: 0.1838 | train_acc: 0.9449 | test_loss: 0.1821 | test_acc: 0.9380\n",
      "Epoch: 564 | train_loss: 0.1893 | train_acc: 0.9422 | test_loss: 0.2188 | test_acc: 0.9296\n",
      "Epoch: 565 | train_loss: 0.2106 | train_acc: 0.9293 | test_loss: 0.2186 | test_acc: 0.9257\n",
      "Epoch: 566 | train_loss: 0.1881 | train_acc: 0.9443 | test_loss: 0.1934 | test_acc: 0.9335\n",
      "Epoch: 567 | train_loss: 0.1877 | train_acc: 0.9412 | test_loss: 0.3011 | test_acc: 0.9238\n",
      "Epoch: 568 | train_loss: 0.2136 | train_acc: 0.9454 | test_loss: 0.2468 | test_acc: 0.9341\n",
      "Epoch: 569 | train_loss: 0.1707 | train_acc: 0.9411 | test_loss: 0.1893 | test_acc: 0.9354\n",
      "Epoch: 570 | train_loss: 0.1425 | train_acc: 0.9546 | test_loss: 0.3058 | test_acc: 0.9399\n",
      "Epoch: 571 | train_loss: 0.1480 | train_acc: 0.9530 | test_loss: 0.2348 | test_acc: 0.9432\n",
      "Epoch: 572 | train_loss: 0.1460 | train_acc: 0.9527 | test_loss: 0.2003 | test_acc: 0.9270\n",
      "Epoch: 573 | train_loss: 0.1286 | train_acc: 0.9587 | test_loss: 0.1925 | test_acc: 0.9406\n",
      "Epoch: 574 | train_loss: 0.1627 | train_acc: 0.9517 | test_loss: 0.2839 | test_acc: 0.9238\n",
      "Epoch: 575 | train_loss: 0.1826 | train_acc: 0.9411 | test_loss: 0.1684 | test_acc: 0.9328\n",
      "Epoch: 576 | train_loss: 0.1575 | train_acc: 0.9459 | test_loss: 0.2668 | test_acc: 0.9380\n",
      "Epoch: 577 | train_loss: 0.2052 | train_acc: 0.9438 | test_loss: 0.2081 | test_acc: 0.9419\n",
      "Epoch: 578 | train_loss: 0.2004 | train_acc: 0.9415 | test_loss: 0.2354 | test_acc: 0.9322\n",
      "Epoch: 579 | train_loss: 0.1914 | train_acc: 0.9449 | test_loss: 0.2047 | test_acc: 0.9367\n",
      "Epoch: 580 | train_loss: 0.1818 | train_acc: 0.9432 | test_loss: 0.2534 | test_acc: 0.9302\n",
      "Epoch: 581 | train_loss: 0.1819 | train_acc: 0.9412 | test_loss: 0.2472 | test_acc: 0.9341\n",
      "Epoch: 582 | train_loss: 0.1513 | train_acc: 0.9530 | test_loss: 0.1824 | test_acc: 0.9477\n",
      "Epoch: 583 | train_loss: 0.1522 | train_acc: 0.9525 | test_loss: 0.2292 | test_acc: 0.9432\n",
      "Epoch: 584 | train_loss: 0.1479 | train_acc: 0.9499 | test_loss: 0.1649 | test_acc: 0.9432\n",
      "Epoch: 585 | train_loss: 0.1579 | train_acc: 0.9556 | test_loss: 0.1907 | test_acc: 0.9367\n",
      "Epoch: 586 | train_loss: 0.1548 | train_acc: 0.9522 | test_loss: 0.2433 | test_acc: 0.9412\n",
      "Epoch: 587 | train_loss: 0.1577 | train_acc: 0.9506 | test_loss: 0.1805 | test_acc: 0.9419\n",
      "Epoch: 588 | train_loss: 0.1525 | train_acc: 0.9495 | test_loss: 0.1563 | test_acc: 0.9464\n",
      "Epoch: 589 | train_loss: 0.1370 | train_acc: 0.9549 | test_loss: 0.1779 | test_acc: 0.9425\n",
      "Epoch: 590 | train_loss: 0.1294 | train_acc: 0.9591 | test_loss: 0.1726 | test_acc: 0.9490\n",
      "Epoch: 591 | train_loss: 0.1423 | train_acc: 0.9541 | test_loss: 0.2429 | test_acc: 0.9134\n",
      "Epoch: 592 | train_loss: 0.1433 | train_acc: 0.9532 | test_loss: 0.1853 | test_acc: 0.9425\n",
      "Epoch: 593 | train_loss: 0.1573 | train_acc: 0.9598 | test_loss: 0.2034 | test_acc: 0.9289\n",
      "Epoch: 594 | train_loss: 0.1485 | train_acc: 0.9549 | test_loss: 0.1780 | test_acc: 0.9380\n",
      "Epoch: 595 | train_loss: 0.1412 | train_acc: 0.9549 | test_loss: 0.1797 | test_acc: 0.9406\n",
      "Epoch: 596 | train_loss: 0.1385 | train_acc: 0.9570 | test_loss: 0.2048 | test_acc: 0.9406\n",
      "Epoch: 597 | train_loss: 0.1463 | train_acc: 0.9530 | test_loss: 0.1712 | test_acc: 0.9425\n",
      "Epoch: 598 | train_loss: 0.1547 | train_acc: 0.9533 | test_loss: 0.2472 | test_acc: 0.9360\n",
      "Epoch: 599 | train_loss: 0.1540 | train_acc: 0.9554 | test_loss: 0.2303 | test_acc: 0.9412\n",
      "Epoch: 600 | train_loss: 0.1814 | train_acc: 0.9512 | test_loss: 0.2327 | test_acc: 0.9238\n",
      "Epoch: 601 | train_loss: 0.1879 | train_acc: 0.9365 | test_loss: 0.1886 | test_acc: 0.9419\n",
      "Epoch: 602 | train_loss: 0.1686 | train_acc: 0.9503 | test_loss: 0.2648 | test_acc: 0.9264\n",
      "Epoch: 603 | train_loss: 0.2284 | train_acc: 0.9365 | test_loss: 0.2344 | test_acc: 0.9289\n",
      "Epoch: 604 | train_loss: 0.1759 | train_acc: 0.9411 | test_loss: 0.2356 | test_acc: 0.9406\n",
      "Epoch: 605 | train_loss: 0.1790 | train_acc: 0.9425 | test_loss: 0.2744 | test_acc: 0.9348\n",
      "Epoch: 606 | train_loss: 0.1682 | train_acc: 0.9506 | test_loss: 0.1688 | test_acc: 0.9367\n",
      "Epoch: 607 | train_loss: 0.1549 | train_acc: 0.9501 | test_loss: 0.1750 | test_acc: 0.9380\n",
      "Epoch: 608 | train_loss: 0.1507 | train_acc: 0.9546 | test_loss: 0.1755 | test_acc: 0.9406\n",
      "Epoch: 609 | train_loss: 0.1472 | train_acc: 0.9533 | test_loss: 0.1936 | test_acc: 0.9425\n",
      "Epoch: 610 | train_loss: 0.1521 | train_acc: 0.9532 | test_loss: 0.1571 | test_acc: 0.9406\n",
      "Epoch: 611 | train_loss: 0.1708 | train_acc: 0.9444 | test_loss: 0.1569 | test_acc: 0.9412\n",
      "Epoch: 612 | train_loss: 0.1886 | train_acc: 0.9433 | test_loss: 0.2559 | test_acc: 0.9360\n",
      "Epoch: 613 | train_loss: 0.1457 | train_acc: 0.9509 | test_loss: 0.2055 | test_acc: 0.9315\n",
      "Epoch: 614 | train_loss: 0.1475 | train_acc: 0.9514 | test_loss: 0.1951 | test_acc: 0.9399\n",
      "Epoch: 615 | train_loss: 0.1465 | train_acc: 0.9532 | test_loss: 0.2244 | test_acc: 0.9419\n",
      "Epoch: 616 | train_loss: 0.1422 | train_acc: 0.9527 | test_loss: 0.2131 | test_acc: 0.9432\n",
      "Epoch: 617 | train_loss: 0.1320 | train_acc: 0.9559 | test_loss: 0.1898 | test_acc: 0.9451\n",
      "Epoch: 618 | train_loss: 0.1361 | train_acc: 0.9562 | test_loss: 0.2036 | test_acc: 0.9399\n",
      "Epoch: 619 | train_loss: 0.1327 | train_acc: 0.9548 | test_loss: 0.1506 | test_acc: 0.9432\n",
      "Epoch: 620 | train_loss: 0.1293 | train_acc: 0.9587 | test_loss: 0.1767 | test_acc: 0.9432\n",
      "Epoch: 621 | train_loss: 0.1574 | train_acc: 0.9557 | test_loss: 0.1713 | test_acc: 0.9406\n",
      "Epoch: 622 | train_loss: 0.1485 | train_acc: 0.9516 | test_loss: 0.2577 | test_acc: 0.9386\n",
      "Epoch: 623 | train_loss: 0.1662 | train_acc: 0.9520 | test_loss: 0.1639 | test_acc: 0.9419\n",
      "Epoch: 624 | train_loss: 0.1572 | train_acc: 0.9475 | test_loss: 0.2411 | test_acc: 0.9438\n",
      "Epoch: 625 | train_loss: 0.1509 | train_acc: 0.9519 | test_loss: 0.1870 | test_acc: 0.9412\n",
      "Epoch: 626 | train_loss: 0.1662 | train_acc: 0.9509 | test_loss: 0.1685 | test_acc: 0.9373\n",
      "Epoch: 627 | train_loss: 0.1418 | train_acc: 0.9546 | test_loss: 0.2114 | test_acc: 0.9341\n",
      "Epoch: 628 | train_loss: 0.1338 | train_acc: 0.9572 | test_loss: 0.1894 | test_acc: 0.9406\n",
      "Epoch: 629 | train_loss: 0.1404 | train_acc: 0.9578 | test_loss: 0.1830 | test_acc: 0.9373\n",
      "Epoch: 630 | train_loss: 0.1487 | train_acc: 0.9549 | test_loss: 0.2017 | test_acc: 0.9399\n",
      "Epoch: 631 | train_loss: 0.1457 | train_acc: 0.9564 | test_loss: 0.5918 | test_acc: 0.9302\n",
      "Epoch: 632 | train_loss: 0.1694 | train_acc: 0.9506 | test_loss: 0.2898 | test_acc: 0.9341\n",
      "Epoch: 633 | train_loss: 0.1602 | train_acc: 0.9488 | test_loss: 0.3424 | test_acc: 0.9289\n",
      "Epoch: 634 | train_loss: 0.1498 | train_acc: 0.9528 | test_loss: 0.3560 | test_acc: 0.9309\n",
      "Epoch: 635 | train_loss: 0.1538 | train_acc: 0.9519 | test_loss: 0.1879 | test_acc: 0.9483\n",
      "Epoch: 636 | train_loss: 0.1500 | train_acc: 0.9511 | test_loss: 0.1688 | test_acc: 0.9386\n",
      "Epoch: 637 | train_loss: 0.1426 | train_acc: 0.9559 | test_loss: 0.1920 | test_acc: 0.9399\n",
      "Epoch: 638 | train_loss: 0.1528 | train_acc: 0.9488 | test_loss: 0.1822 | test_acc: 0.9432\n",
      "Epoch: 639 | train_loss: 0.1580 | train_acc: 0.9549 | test_loss: 0.2956 | test_acc: 0.9302\n",
      "Epoch: 640 | train_loss: 0.1815 | train_acc: 0.9420 | test_loss: 0.1805 | test_acc: 0.9425\n",
      "Epoch: 641 | train_loss: 0.1518 | train_acc: 0.9516 | test_loss: 0.3455 | test_acc: 0.9386\n",
      "Epoch: 642 | train_loss: 0.1539 | train_acc: 0.9533 | test_loss: 0.1594 | test_acc: 0.9444\n",
      "Epoch: 643 | train_loss: 0.1439 | train_acc: 0.9549 | test_loss: 0.2444 | test_acc: 0.9360\n",
      "Epoch: 644 | train_loss: 0.1321 | train_acc: 0.9583 | test_loss: 0.1568 | test_acc: 0.9483\n",
      "Epoch: 645 | train_loss: 0.1544 | train_acc: 0.9566 | test_loss: 0.4809 | test_acc: 0.9373\n",
      "Epoch: 646 | train_loss: 0.1507 | train_acc: 0.9504 | test_loss: 0.2243 | test_acc: 0.9367\n",
      "Epoch: 647 | train_loss: 0.1662 | train_acc: 0.9538 | test_loss: 0.2149 | test_acc: 0.9289\n",
      "Epoch: 648 | train_loss: 0.1717 | train_acc: 0.9512 | test_loss: 0.2423 | test_acc: 0.9121\n",
      "Epoch: 649 | train_loss: 0.1391 | train_acc: 0.9511 | test_loss: 0.2182 | test_acc: 0.9225\n",
      "Epoch: 650 | train_loss: 0.1376 | train_acc: 0.9554 | test_loss: 0.3791 | test_acc: 0.9315\n",
      "Epoch: 651 | train_loss: 0.1634 | train_acc: 0.9545 | test_loss: 0.2662 | test_acc: 0.9354\n",
      "Epoch: 652 | train_loss: 0.1660 | train_acc: 0.9509 | test_loss: 0.1978 | test_acc: 0.9425\n",
      "Epoch: 653 | train_loss: 0.1808 | train_acc: 0.9464 | test_loss: 0.1917 | test_acc: 0.9399\n",
      "Epoch: 654 | train_loss: 0.1642 | train_acc: 0.9503 | test_loss: 0.1591 | test_acc: 0.9406\n",
      "Epoch: 655 | train_loss: 0.1499 | train_acc: 0.9501 | test_loss: 0.2342 | test_acc: 0.9367\n",
      "Epoch: 656 | train_loss: 0.1582 | train_acc: 0.9524 | test_loss: 0.3596 | test_acc: 0.9386\n",
      "Epoch: 657 | train_loss: 0.1695 | train_acc: 0.9503 | test_loss: 0.2156 | test_acc: 0.9302\n",
      "[INFO] Saving model to: models\\ECG_Net_for_5_95.03%.pth\n",
      "Epoch: 658 | train_loss: 0.1640 | train_acc: 0.9477 | test_loss: 0.1579 | test_acc: 0.9503\n",
      "Epoch: 659 | train_loss: 0.1489 | train_acc: 0.9532 | test_loss: 0.1869 | test_acc: 0.9335\n",
      "Epoch: 660 | train_loss: 0.1469 | train_acc: 0.9535 | test_loss: 0.1896 | test_acc: 0.9399\n",
      "Epoch: 661 | train_loss: 0.1506 | train_acc: 0.9541 | test_loss: 0.1975 | test_acc: 0.9470\n",
      "Epoch: 662 | train_loss: 0.1521 | train_acc: 0.9519 | test_loss: 0.3259 | test_acc: 0.9438\n",
      "Epoch: 663 | train_loss: 0.1722 | train_acc: 0.9514 | test_loss: 0.2390 | test_acc: 0.9393\n",
      "Epoch: 664 | train_loss: 0.1483 | train_acc: 0.9519 | test_loss: 0.1765 | test_acc: 0.9354\n",
      "Epoch: 665 | train_loss: 0.1730 | train_acc: 0.9527 | test_loss: 0.1849 | test_acc: 0.9490\n",
      "Epoch: 666 | train_loss: 0.1545 | train_acc: 0.9514 | test_loss: 0.2101 | test_acc: 0.9393\n",
      "Epoch: 667 | train_loss: 0.1529 | train_acc: 0.9540 | test_loss: 0.4532 | test_acc: 0.9393\n",
      "Epoch: 668 | train_loss: 0.1481 | train_acc: 0.9530 | test_loss: 0.1866 | test_acc: 0.9360\n",
      "Epoch: 669 | train_loss: 0.1393 | train_acc: 0.9591 | test_loss: 0.1864 | test_acc: 0.9444\n",
      "Epoch: 670 | train_loss: 0.1568 | train_acc: 0.9520 | test_loss: 0.1967 | test_acc: 0.9503\n",
      "Epoch: 671 | train_loss: 0.1416 | train_acc: 0.9566 | test_loss: 0.1820 | test_acc: 0.9432\n",
      "Epoch: 672 | train_loss: 0.1235 | train_acc: 0.9588 | test_loss: 0.1610 | test_acc: 0.9399\n",
      "Epoch: 673 | train_loss: 0.1467 | train_acc: 0.9540 | test_loss: 0.2051 | test_acc: 0.9477\n",
      "Epoch: 674 | train_loss: 0.1682 | train_acc: 0.9562 | test_loss: 0.3680 | test_acc: 0.9406\n",
      "Epoch: 675 | train_loss: 0.1748 | train_acc: 0.9469 | test_loss: 0.3788 | test_acc: 0.9315\n",
      "Epoch: 676 | train_loss: 0.1501 | train_acc: 0.9553 | test_loss: 0.2995 | test_acc: 0.9438\n",
      "Epoch: 677 | train_loss: 0.1361 | train_acc: 0.9593 | test_loss: 0.2442 | test_acc: 0.9451\n",
      "Epoch: 678 | train_loss: 0.1232 | train_acc: 0.9595 | test_loss: 0.1641 | test_acc: 0.9412\n",
      "Epoch: 679 | train_loss: 0.1305 | train_acc: 0.9561 | test_loss: 0.2571 | test_acc: 0.9464\n",
      "Epoch: 680 | train_loss: 0.1253 | train_acc: 0.9591 | test_loss: 0.4610 | test_acc: 0.9483\n",
      "Epoch: 681 | train_loss: 0.1303 | train_acc: 0.9577 | test_loss: 0.1974 | test_acc: 0.9483\n",
      "Epoch: 682 | train_loss: 0.1652 | train_acc: 0.9548 | test_loss: 0.2138 | test_acc: 0.9406\n",
      "Epoch: 683 | train_loss: 0.1948 | train_acc: 0.9433 | test_loss: 0.2008 | test_acc: 0.9412\n",
      "Epoch: 684 | train_loss: 0.1828 | train_acc: 0.9420 | test_loss: 0.1939 | test_acc: 0.9328\n",
      "Epoch: 685 | train_loss: 0.1408 | train_acc: 0.9556 | test_loss: 0.3286 | test_acc: 0.9419\n",
      "Epoch: 686 | train_loss: 0.1334 | train_acc: 0.9580 | test_loss: 0.1689 | test_acc: 0.9464\n",
      "Epoch: 687 | train_loss: 0.1323 | train_acc: 0.9577 | test_loss: 0.2518 | test_acc: 0.9425\n",
      "Epoch: 688 | train_loss: 0.1313 | train_acc: 0.9577 | test_loss: 0.1597 | test_acc: 0.9451\n",
      "Epoch: 689 | train_loss: 0.1316 | train_acc: 0.9567 | test_loss: 0.1608 | test_acc: 0.9438\n",
      "Epoch: 690 | train_loss: 0.1302 | train_acc: 0.9614 | test_loss: 0.1790 | test_acc: 0.9438\n",
      "Epoch: 691 | train_loss: 0.1378 | train_acc: 0.9570 | test_loss: 0.1837 | test_acc: 0.9406\n",
      "Epoch: 692 | train_loss: 0.1415 | train_acc: 0.9543 | test_loss: 0.1551 | test_acc: 0.9503\n",
      "Epoch: 693 | train_loss: 0.1442 | train_acc: 0.9578 | test_loss: 0.1661 | test_acc: 0.9386\n",
      "Epoch: 694 | train_loss: 0.1590 | train_acc: 0.9470 | test_loss: 0.2933 | test_acc: 0.9289\n",
      "Epoch: 695 | train_loss: 0.1495 | train_acc: 0.9517 | test_loss: 0.2595 | test_acc: 0.9464\n",
      "Epoch: 696 | train_loss: 0.1450 | train_acc: 0.9569 | test_loss: 0.1848 | test_acc: 0.9367\n",
      "Epoch: 697 | train_loss: 0.1376 | train_acc: 0.9535 | test_loss: 0.2069 | test_acc: 0.9393\n",
      "Epoch: 698 | train_loss: 0.1361 | train_acc: 0.9549 | test_loss: 0.3512 | test_acc: 0.9367\n",
      "Epoch: 699 | train_loss: 0.1611 | train_acc: 0.9493 | test_loss: 0.2067 | test_acc: 0.9276\n",
      "Epoch: 700 | train_loss: 0.1396 | train_acc: 0.9525 | test_loss: 0.1580 | test_acc: 0.9367\n",
      "Epoch: 701 | train_loss: 0.1471 | train_acc: 0.9569 | test_loss: 0.1977 | test_acc: 0.9380\n",
      "Epoch: 702 | train_loss: 0.1327 | train_acc: 0.9566 | test_loss: 0.2172 | test_acc: 0.9367\n",
      "Epoch: 703 | train_loss: 0.1306 | train_acc: 0.9587 | test_loss: 0.1889 | test_acc: 0.9451\n",
      "Epoch: 704 | train_loss: 0.1410 | train_acc: 0.9559 | test_loss: 0.1927 | test_acc: 0.9406\n",
      "Epoch: 705 | train_loss: 0.1491 | train_acc: 0.9525 | test_loss: 0.1654 | test_acc: 0.9406\n",
      "Epoch: 706 | train_loss: 0.1432 | train_acc: 0.9530 | test_loss: 0.1738 | test_acc: 0.9470\n",
      "Epoch: 707 | train_loss: 0.1218 | train_acc: 0.9578 | test_loss: 0.2589 | test_acc: 0.9315\n",
      "Epoch: 708 | train_loss: 0.1208 | train_acc: 0.9603 | test_loss: 0.1747 | test_acc: 0.9406\n",
      "Epoch: 709 | train_loss: 0.1357 | train_acc: 0.9580 | test_loss: 0.3486 | test_acc: 0.9470\n",
      "Epoch: 710 | train_loss: 0.1720 | train_acc: 0.9536 | test_loss: 0.1888 | test_acc: 0.9419\n",
      "Epoch: 711 | train_loss: 0.1664 | train_acc: 0.9478 | test_loss: 0.2423 | test_acc: 0.9335\n",
      "Epoch: 712 | train_loss: 0.2181 | train_acc: 0.9369 | test_loss: 0.1800 | test_acc: 0.9348\n",
      "Epoch: 713 | train_loss: 0.1955 | train_acc: 0.9351 | test_loss: 0.1830 | test_acc: 0.9393\n",
      "Epoch: 714 | train_loss: 0.1773 | train_acc: 0.9444 | test_loss: 0.2269 | test_acc: 0.9283\n",
      "Epoch: 715 | train_loss: 0.1594 | train_acc: 0.9467 | test_loss: 0.1705 | test_acc: 0.9457\n",
      "Epoch: 716 | train_loss: 0.1623 | train_acc: 0.9528 | test_loss: 0.1845 | test_acc: 0.9451\n",
      "Epoch: 717 | train_loss: 0.1474 | train_acc: 0.9514 | test_loss: 0.1972 | test_acc: 0.9419\n",
      "[INFO] Saving model to: models\\ECG_Net_for_5_95.09%.pth\n",
      "Epoch: 718 | train_loss: 0.1253 | train_acc: 0.9583 | test_loss: 0.2087 | test_acc: 0.9509\n",
      "Epoch: 719 | train_loss: 0.1470 | train_acc: 0.9559 | test_loss: 0.2786 | test_acc: 0.9328\n",
      "Epoch: 720 | train_loss: 0.1415 | train_acc: 0.9553 | test_loss: 0.1706 | test_acc: 0.9432\n",
      "Epoch: 721 | train_loss: 0.1445 | train_acc: 0.9566 | test_loss: 0.1691 | test_acc: 0.9444\n",
      "Epoch: 722 | train_loss: 0.1412 | train_acc: 0.9548 | test_loss: 0.1753 | test_acc: 0.9451\n",
      "Epoch: 723 | train_loss: 0.1386 | train_acc: 0.9549 | test_loss: 0.2238 | test_acc: 0.9490\n",
      "Epoch: 724 | train_loss: 0.1265 | train_acc: 0.9598 | test_loss: 0.3555 | test_acc: 0.9432\n",
      "Epoch: 725 | train_loss: 0.1428 | train_acc: 0.9580 | test_loss: 0.1638 | test_acc: 0.9496\n",
      "Epoch: 726 | train_loss: 0.1598 | train_acc: 0.9567 | test_loss: 0.2272 | test_acc: 0.9289\n",
      "Epoch: 727 | train_loss: 0.1558 | train_acc: 0.9509 | test_loss: 0.1979 | test_acc: 0.9373\n",
      "Epoch: 728 | train_loss: 0.1612 | train_acc: 0.9520 | test_loss: 0.2346 | test_acc: 0.9470\n",
      "Epoch: 729 | train_loss: 0.1390 | train_acc: 0.9549 | test_loss: 0.4916 | test_acc: 0.9380\n",
      "Epoch: 730 | train_loss: 0.1289 | train_acc: 0.9561 | test_loss: 0.1509 | test_acc: 0.9380\n",
      "Epoch: 731 | train_loss: 0.1270 | train_acc: 0.9596 | test_loss: 0.1536 | test_acc: 0.9477\n",
      "Epoch: 732 | train_loss: 0.1390 | train_acc: 0.9567 | test_loss: 0.1690 | test_acc: 0.9373\n",
      "Epoch: 733 | train_loss: 0.1427 | train_acc: 0.9545 | test_loss: 0.1650 | test_acc: 0.9406\n",
      "Epoch: 734 | train_loss: 0.1299 | train_acc: 0.9574 | test_loss: 0.1753 | test_acc: 0.9373\n",
      "Epoch: 735 | train_loss: 0.1251 | train_acc: 0.9587 | test_loss: 0.1584 | test_acc: 0.9451\n",
      "Epoch: 736 | train_loss: 0.1393 | train_acc: 0.9545 | test_loss: 0.2147 | test_acc: 0.9322\n",
      "Epoch: 737 | train_loss: 0.1384 | train_acc: 0.9527 | test_loss: 0.1517 | test_acc: 0.9380\n",
      "Epoch: 738 | train_loss: 0.1252 | train_acc: 0.9595 | test_loss: 0.2739 | test_acc: 0.9348\n",
      "Epoch: 739 | train_loss: 0.1301 | train_acc: 0.9572 | test_loss: 0.1723 | test_acc: 0.9367\n",
      "Epoch: 740 | train_loss: 0.1343 | train_acc: 0.9580 | test_loss: 0.3937 | test_acc: 0.9419\n",
      "Epoch: 741 | train_loss: 0.1291 | train_acc: 0.9577 | test_loss: 0.2296 | test_acc: 0.9483\n",
      "Epoch: 742 | train_loss: 0.1363 | train_acc: 0.9578 | test_loss: 0.1829 | test_acc: 0.9438\n",
      "Epoch: 743 | train_loss: 0.1510 | train_acc: 0.9569 | test_loss: 0.1635 | test_acc: 0.9470\n",
      "Epoch: 744 | train_loss: 0.1633 | train_acc: 0.9477 | test_loss: 0.2282 | test_acc: 0.9141\n",
      "Epoch: 745 | train_loss: 0.1486 | train_acc: 0.9525 | test_loss: 0.1997 | test_acc: 0.9328\n",
      "Epoch: 746 | train_loss: 0.1451 | train_acc: 0.9593 | test_loss: 0.2565 | test_acc: 0.9470\n",
      "Epoch: 747 | train_loss: 0.1390 | train_acc: 0.9574 | test_loss: 0.2771 | test_acc: 0.9406\n",
      "Epoch: 748 | train_loss: 0.1468 | train_acc: 0.9557 | test_loss: 0.1854 | test_acc: 0.9438\n",
      "Epoch: 749 | train_loss: 0.1293 | train_acc: 0.9559 | test_loss: 0.2236 | test_acc: 0.9264\n",
      "Epoch: 750 | train_loss: 0.1338 | train_acc: 0.9559 | test_loss: 0.4715 | test_acc: 0.9367\n",
      "Epoch: 751 | train_loss: 0.1232 | train_acc: 0.9588 | test_loss: 0.1900 | test_acc: 0.9348\n",
      "[INFO] Saving model to: models\\ECG_Net_for_5_95.35%.pth\n",
      "Epoch: 752 | train_loss: 0.1332 | train_acc: 0.9564 | test_loss: 0.1504 | test_acc: 0.9535\n",
      "Epoch: 753 | train_loss: 0.1270 | train_acc: 0.9604 | test_loss: 0.1737 | test_acc: 0.9419\n",
      "Epoch: 754 | train_loss: 0.1394 | train_acc: 0.9567 | test_loss: 0.3085 | test_acc: 0.9432\n",
      "Epoch: 755 | train_loss: 0.1474 | train_acc: 0.9583 | test_loss: 0.1755 | test_acc: 0.9438\n",
      "Epoch: 756 | train_loss: 0.1292 | train_acc: 0.9617 | test_loss: 0.1793 | test_acc: 0.9516\n",
      "Epoch: 757 | train_loss: 0.1464 | train_acc: 0.9562 | test_loss: 0.2217 | test_acc: 0.9444\n",
      "Epoch: 758 | train_loss: 0.1434 | train_acc: 0.9582 | test_loss: 0.4364 | test_acc: 0.9367\n",
      "Epoch: 759 | train_loss: 0.1436 | train_acc: 0.9569 | test_loss: 0.2178 | test_acc: 0.9464\n",
      "Epoch: 760 | train_loss: 0.1834 | train_acc: 0.9490 | test_loss: 0.1936 | test_acc: 0.9451\n",
      "Epoch: 761 | train_loss: 0.1663 | train_acc: 0.9483 | test_loss: 0.3896 | test_acc: 0.9089\n",
      "Epoch: 762 | train_loss: 0.1408 | train_acc: 0.9551 | test_loss: 0.1671 | test_acc: 0.9477\n",
      "Epoch: 763 | train_loss: 0.1285 | train_acc: 0.9593 | test_loss: 0.2948 | test_acc: 0.9457\n",
      "Epoch: 764 | train_loss: 0.1548 | train_acc: 0.9514 | test_loss: 0.4125 | test_acc: 0.9238\n",
      "Epoch: 765 | train_loss: 0.1587 | train_acc: 0.9512 | test_loss: 0.2120 | test_acc: 0.9406\n",
      "Epoch: 766 | train_loss: 0.1373 | train_acc: 0.9548 | test_loss: 0.1964 | test_acc: 0.9425\n",
      "Epoch: 767 | train_loss: 0.1257 | train_acc: 0.9622 | test_loss: 0.1687 | test_acc: 0.9432\n",
      "Epoch: 768 | train_loss: 0.1292 | train_acc: 0.9596 | test_loss: 0.3000 | test_acc: 0.9432\n",
      "Epoch: 769 | train_loss: 0.1251 | train_acc: 0.9608 | test_loss: 0.1721 | test_acc: 0.9464\n",
      "Epoch: 770 | train_loss: 0.1570 | train_acc: 0.9567 | test_loss: 0.2155 | test_acc: 0.9483\n",
      "Epoch: 771 | train_loss: 0.1682 | train_acc: 0.9509 | test_loss: 0.1726 | test_acc: 0.9406\n",
      "Epoch: 772 | train_loss: 0.1466 | train_acc: 0.9546 | test_loss: 0.2344 | test_acc: 0.9393\n",
      "Epoch: 773 | train_loss: 0.1270 | train_acc: 0.9578 | test_loss: 0.1710 | test_acc: 0.9419\n",
      "Epoch: 774 | train_loss: 0.1209 | train_acc: 0.9608 | test_loss: 0.1654 | test_acc: 0.9432\n",
      "Epoch: 775 | train_loss: 0.1265 | train_acc: 0.9611 | test_loss: 0.1646 | test_acc: 0.9490\n",
      "Epoch: 776 | train_loss: 0.1370 | train_acc: 0.9612 | test_loss: 0.3825 | test_acc: 0.9264\n",
      "Epoch: 777 | train_loss: 0.1679 | train_acc: 0.9522 | test_loss: 0.1860 | test_acc: 0.9380\n",
      "Epoch: 778 | train_loss: 0.1887 | train_acc: 0.9462 | test_loss: 0.2426 | test_acc: 0.9193\n",
      "Epoch: 779 | train_loss: 0.1866 | train_acc: 0.9448 | test_loss: 0.2862 | test_acc: 0.9341\n",
      "Epoch: 780 | train_loss: 0.1440 | train_acc: 0.9507 | test_loss: 0.1760 | test_acc: 0.9367\n",
      "Epoch: 781 | train_loss: 0.1518 | train_acc: 0.9517 | test_loss: 0.1704 | test_acc: 0.9464\n",
      "Epoch: 782 | train_loss: 0.1348 | train_acc: 0.9606 | test_loss: 0.1464 | test_acc: 0.9496\n",
      "Epoch: 783 | train_loss: 0.1365 | train_acc: 0.9554 | test_loss: 0.3983 | test_acc: 0.9328\n",
      "Epoch: 784 | train_loss: 0.1204 | train_acc: 0.9595 | test_loss: 0.2924 | test_acc: 0.9457\n",
      "Epoch: 785 | train_loss: 0.1270 | train_acc: 0.9601 | test_loss: 0.1801 | test_acc: 0.9444\n",
      "Epoch: 786 | train_loss: 0.1351 | train_acc: 0.9574 | test_loss: 0.2768 | test_acc: 0.9419\n",
      "Epoch: 787 | train_loss: 0.1332 | train_acc: 0.9598 | test_loss: 0.1467 | test_acc: 0.9464\n",
      "Epoch: 788 | train_loss: 0.1305 | train_acc: 0.9553 | test_loss: 0.1621 | test_acc: 0.9425\n",
      "Epoch: 789 | train_loss: 0.1277 | train_acc: 0.9599 | test_loss: 0.1630 | test_acc: 0.9451\n",
      "Epoch: 790 | train_loss: 0.1190 | train_acc: 0.9601 | test_loss: 0.1825 | test_acc: 0.9444\n",
      "Epoch: 791 | train_loss: 0.1278 | train_acc: 0.9593 | test_loss: 0.1715 | test_acc: 0.9367\n",
      "Epoch: 792 | train_loss: 0.1231 | train_acc: 0.9590 | test_loss: 0.1615 | test_acc: 0.9451\n",
      "Epoch: 793 | train_loss: 0.1336 | train_acc: 0.9630 | test_loss: 0.2199 | test_acc: 0.9335\n",
      "Epoch: 794 | train_loss: 0.1136 | train_acc: 0.9640 | test_loss: 0.1859 | test_acc: 0.9503\n",
      "Epoch: 795 | train_loss: 0.1344 | train_acc: 0.9591 | test_loss: 0.1739 | test_acc: 0.9451\n",
      "Epoch: 796 | train_loss: 0.1433 | train_acc: 0.9541 | test_loss: 0.2061 | test_acc: 0.9354\n",
      "Epoch: 797 | train_loss: 0.1292 | train_acc: 0.9599 | test_loss: 0.2261 | test_acc: 0.9438\n",
      "Epoch: 798 | train_loss: 0.1404 | train_acc: 0.9549 | test_loss: 0.1447 | test_acc: 0.9470\n",
      "Epoch: 799 | train_loss: 0.1284 | train_acc: 0.9593 | test_loss: 0.2450 | test_acc: 0.9457\n",
      "Epoch: 800 | train_loss: 0.1354 | train_acc: 0.9580 | test_loss: 0.2392 | test_acc: 0.9406\n",
      "Epoch: 801 | train_loss: 0.1183 | train_acc: 0.9650 | test_loss: 0.1762 | test_acc: 0.9509\n",
      "Epoch: 802 | train_loss: 0.1261 | train_acc: 0.9566 | test_loss: 0.1837 | test_acc: 0.9425\n",
      "Epoch: 803 | train_loss: 0.1316 | train_acc: 0.9570 | test_loss: 0.2903 | test_acc: 0.9393\n",
      "Epoch: 804 | train_loss: 0.1312 | train_acc: 0.9616 | test_loss: 0.1697 | test_acc: 0.9432\n",
      "Epoch: 805 | train_loss: 0.1209 | train_acc: 0.9646 | test_loss: 0.3457 | test_acc: 0.9451\n",
      "Epoch: 806 | train_loss: 0.1147 | train_acc: 0.9614 | test_loss: 0.1782 | test_acc: 0.9470\n",
      "Epoch: 807 | train_loss: 0.1194 | train_acc: 0.9632 | test_loss: 0.3274 | test_acc: 0.9322\n",
      "Epoch: 808 | train_loss: 0.1368 | train_acc: 0.9564 | test_loss: 0.1976 | test_acc: 0.9444\n",
      "Epoch: 809 | train_loss: 0.1277 | train_acc: 0.9575 | test_loss: 0.3633 | test_acc: 0.9425\n",
      "Epoch: 810 | train_loss: 0.1254 | train_acc: 0.9601 | test_loss: 0.1473 | test_acc: 0.9470\n",
      "Epoch: 811 | train_loss: 0.1258 | train_acc: 0.9608 | test_loss: 0.1712 | test_acc: 0.9438\n",
      "Epoch: 812 | train_loss: 0.1221 | train_acc: 0.9624 | test_loss: 0.3053 | test_acc: 0.9451\n",
      "Epoch: 813 | train_loss: 0.1255 | train_acc: 0.9619 | test_loss: 0.1705 | test_acc: 0.9380\n",
      "Epoch: 814 | train_loss: 0.1315 | train_acc: 0.9580 | test_loss: 0.2192 | test_acc: 0.9412\n",
      "Epoch: 815 | train_loss: 0.1369 | train_acc: 0.9569 | test_loss: 0.1621 | test_acc: 0.9444\n",
      "Epoch: 816 | train_loss: 0.1267 | train_acc: 0.9619 | test_loss: 0.2018 | test_acc: 0.9419\n",
      "Epoch: 817 | train_loss: 0.1197 | train_acc: 0.9590 | test_loss: 0.3512 | test_acc: 0.9457\n",
      "Epoch: 818 | train_loss: 0.1256 | train_acc: 0.9578 | test_loss: 0.2757 | test_acc: 0.9444\n",
      "Epoch: 819 | train_loss: 0.1258 | train_acc: 0.9567 | test_loss: 0.1877 | test_acc: 0.9419\n",
      "Epoch: 820 | train_loss: 0.1243 | train_acc: 0.9577 | test_loss: 0.2010 | test_acc: 0.9432\n",
      "Epoch: 821 | train_loss: 0.1187 | train_acc: 0.9630 | test_loss: 0.1793 | test_acc: 0.9451\n",
      "Epoch: 822 | train_loss: 0.1393 | train_acc: 0.9601 | test_loss: 0.1675 | test_acc: 0.9419\n",
      "Epoch: 823 | train_loss: 0.1423 | train_acc: 0.9530 | test_loss: 0.1914 | test_acc: 0.9380\n",
      "Epoch: 824 | train_loss: 0.1292 | train_acc: 0.9536 | test_loss: 0.2154 | test_acc: 0.9425\n",
      "Epoch: 825 | train_loss: 0.1460 | train_acc: 0.9593 | test_loss: 0.4149 | test_acc: 0.9412\n",
      "Epoch: 826 | train_loss: 0.1761 | train_acc: 0.9495 | test_loss: 0.2508 | test_acc: 0.9393\n",
      "Epoch: 827 | train_loss: 0.1819 | train_acc: 0.9454 | test_loss: 0.2418 | test_acc: 0.9341\n",
      "Epoch: 828 | train_loss: 0.1634 | train_acc: 0.9478 | test_loss: 0.2054 | test_acc: 0.9399\n",
      "Epoch: 829 | train_loss: 0.1640 | train_acc: 0.9525 | test_loss: 0.5213 | test_acc: 0.9444\n",
      "Epoch: 830 | train_loss: 0.1847 | train_acc: 0.9467 | test_loss: 0.3128 | test_acc: 0.9406\n",
      "Epoch: 831 | train_loss: 0.1718 | train_acc: 0.9482 | test_loss: 0.3649 | test_acc: 0.9360\n",
      "Epoch: 832 | train_loss: 0.1532 | train_acc: 0.9491 | test_loss: 0.1872 | test_acc: 0.9335\n",
      "Epoch: 833 | train_loss: 0.1677 | train_acc: 0.9556 | test_loss: 0.1896 | test_acc: 0.9360\n",
      "Epoch: 834 | train_loss: 0.1685 | train_acc: 0.9520 | test_loss: 0.1929 | test_acc: 0.9419\n",
      "Epoch: 835 | train_loss: 0.1438 | train_acc: 0.9520 | test_loss: 0.1654 | test_acc: 0.9393\n",
      "Epoch: 836 | train_loss: 0.1274 | train_acc: 0.9614 | test_loss: 0.2168 | test_acc: 0.9419\n",
      "Epoch: 837 | train_loss: 0.1202 | train_acc: 0.9611 | test_loss: 0.2208 | test_acc: 0.9354\n",
      "Epoch: 838 | train_loss: 0.1452 | train_acc: 0.9564 | test_loss: 0.3982 | test_acc: 0.9470\n",
      "Epoch: 839 | train_loss: 0.1503 | train_acc: 0.9516 | test_loss: 0.4749 | test_acc: 0.9386\n",
      "Epoch: 840 | train_loss: 0.1316 | train_acc: 0.9548 | test_loss: 0.1758 | test_acc: 0.9393\n",
      "Epoch: 841 | train_loss: 0.1338 | train_acc: 0.9549 | test_loss: 0.4120 | test_acc: 0.9335\n",
      "Epoch: 842 | train_loss: 0.1442 | train_acc: 0.9562 | test_loss: 0.1593 | test_acc: 0.9516\n",
      "Epoch: 843 | train_loss: 0.1305 | train_acc: 0.9587 | test_loss: 0.2564 | test_acc: 0.9444\n",
      "Epoch: 844 | train_loss: 0.1341 | train_acc: 0.9567 | test_loss: 0.1548 | test_acc: 0.9496\n",
      "Epoch: 845 | train_loss: 0.1299 | train_acc: 0.9612 | test_loss: 0.2367 | test_acc: 0.9425\n",
      "Epoch: 846 | train_loss: 0.1303 | train_acc: 0.9599 | test_loss: 0.1831 | test_acc: 0.9477\n",
      "Epoch: 847 | train_loss: 0.1359 | train_acc: 0.9570 | test_loss: 0.2232 | test_acc: 0.9444\n",
      "Epoch: 848 | train_loss: 0.1235 | train_acc: 0.9598 | test_loss: 0.1693 | test_acc: 0.9470\n",
      "Epoch: 849 | train_loss: 0.1194 | train_acc: 0.9564 | test_loss: 0.1904 | test_acc: 0.9464\n",
      "Epoch: 850 | train_loss: 0.1320 | train_acc: 0.9604 | test_loss: 0.1728 | test_acc: 0.9516\n",
      "Epoch: 851 | train_loss: 0.1193 | train_acc: 0.9637 | test_loss: 0.2102 | test_acc: 0.9386\n",
      "Epoch: 852 | train_loss: 0.1522 | train_acc: 0.9535 | test_loss: 0.1897 | test_acc: 0.9399\n",
      "Epoch: 853 | train_loss: 0.1353 | train_acc: 0.9656 | test_loss: 0.1577 | test_acc: 0.9419\n",
      "Epoch: 854 | train_loss: 0.1404 | train_acc: 0.9512 | test_loss: 0.1717 | test_acc: 0.9483\n",
      "Epoch: 855 | train_loss: 0.1636 | train_acc: 0.9490 | test_loss: 0.3198 | test_acc: 0.9380\n",
      "Epoch: 856 | train_loss: 0.1366 | train_acc: 0.9556 | test_loss: 0.2041 | test_acc: 0.9360\n",
      "Epoch: 857 | train_loss: 0.1337 | train_acc: 0.9599 | test_loss: 0.1785 | test_acc: 0.9360\n",
      "Epoch: 858 | train_loss: 0.1363 | train_acc: 0.9554 | test_loss: 0.1763 | test_acc: 0.9425\n",
      "Epoch: 859 | train_loss: 0.1176 | train_acc: 0.9625 | test_loss: 0.3140 | test_acc: 0.9457\n",
      "Epoch: 860 | train_loss: 0.1176 | train_acc: 0.9588 | test_loss: 0.2024 | test_acc: 0.9367\n",
      "Epoch: 861 | train_loss: 0.1325 | train_acc: 0.9620 | test_loss: 0.1707 | test_acc: 0.9470\n",
      "Epoch: 862 | train_loss: 0.1507 | train_acc: 0.9553 | test_loss: 0.2451 | test_acc: 0.9451\n",
      "Epoch: 863 | train_loss: 0.1318 | train_acc: 0.9554 | test_loss: 0.1887 | test_acc: 0.9309\n",
      "Epoch: 864 | train_loss: 0.1667 | train_acc: 0.9451 | test_loss: 0.4450 | test_acc: 0.9322\n",
      "Epoch: 865 | train_loss: 0.1463 | train_acc: 0.9501 | test_loss: 0.1905 | test_acc: 0.9412\n",
      "Epoch: 866 | train_loss: 0.1352 | train_acc: 0.9557 | test_loss: 0.1649 | test_acc: 0.9425\n",
      "Epoch: 867 | train_loss: 0.1297 | train_acc: 0.9575 | test_loss: 0.2811 | test_acc: 0.9432\n",
      "Epoch: 868 | train_loss: 0.1487 | train_acc: 0.9609 | test_loss: 0.1682 | test_acc: 0.9451\n",
      "Epoch: 869 | train_loss: 0.1413 | train_acc: 0.9564 | test_loss: 0.3646 | test_acc: 0.9393\n",
      "Epoch: 870 | train_loss: 0.1515 | train_acc: 0.9538 | test_loss: 0.2689 | test_acc: 0.9231\n",
      "Epoch: 871 | train_loss: 0.1634 | train_acc: 0.9496 | test_loss: 0.2608 | test_acc: 0.9399\n",
      "Epoch: 872 | train_loss: 0.1879 | train_acc: 0.9488 | test_loss: 0.1827 | test_acc: 0.9335\n",
      "Epoch: 873 | train_loss: 0.1846 | train_acc: 0.9498 | test_loss: 0.2041 | test_acc: 0.9354\n",
      "Epoch: 874 | train_loss: 0.1896 | train_acc: 0.9423 | test_loss: 0.2292 | test_acc: 0.9276\n",
      "Epoch: 875 | train_loss: 0.1702 | train_acc: 0.9432 | test_loss: 0.1897 | test_acc: 0.9412\n",
      "Epoch: 876 | train_loss: 0.1608 | train_acc: 0.9490 | test_loss: 0.2937 | test_acc: 0.9270\n",
      "Epoch: 877 | train_loss: 0.1958 | train_acc: 0.9461 | test_loss: 0.1725 | test_acc: 0.9464\n",
      "Epoch: 878 | train_loss: 0.2511 | train_acc: 0.9281 | test_loss: 0.4463 | test_acc: 0.8798\n",
      "Epoch: 879 | train_loss: 0.2915 | train_acc: 0.9168 | test_loss: 0.2668 | test_acc: 0.9005\n",
      "Epoch: 880 | train_loss: 0.3067 | train_acc: 0.9055 | test_loss: 0.2967 | test_acc: 0.8953\n",
      "Epoch: 881 | train_loss: 0.3135 | train_acc: 0.8994 | test_loss: 0.3729 | test_acc: 0.8928\n",
      "Epoch: 882 | train_loss: 0.2490 | train_acc: 0.9176 | test_loss: 0.2593 | test_acc: 0.9089\n",
      "Epoch: 883 | train_loss: 0.2353 | train_acc: 0.9318 | test_loss: 0.1696 | test_acc: 0.9328\n",
      "Epoch: 884 | train_loss: 0.2132 | train_acc: 0.9328 | test_loss: 0.2110 | test_acc: 0.9244\n",
      "Epoch: 885 | train_loss: 0.1894 | train_acc: 0.9344 | test_loss: 0.1851 | test_acc: 0.9296\n",
      "Epoch: 886 | train_loss: 0.2017 | train_acc: 0.9370 | test_loss: 0.1860 | test_acc: 0.9348\n",
      "Epoch: 887 | train_loss: 0.2138 | train_acc: 0.9330 | test_loss: 0.2261 | test_acc: 0.9348\n",
      "Epoch: 888 | train_loss: 0.2420 | train_acc: 0.9249 | test_loss: 0.4517 | test_acc: 0.8953\n",
      "Epoch: 889 | train_loss: 0.2446 | train_acc: 0.9231 | test_loss: 0.2349 | test_acc: 0.9244\n",
      "Epoch: 890 | train_loss: 0.2426 | train_acc: 0.9231 | test_loss: 0.3245 | test_acc: 0.9257\n",
      "Epoch: 891 | train_loss: 0.2345 | train_acc: 0.9252 | test_loss: 0.2525 | test_acc: 0.9231\n",
      "Epoch: 892 | train_loss: 0.2412 | train_acc: 0.9272 | test_loss: 0.4983 | test_acc: 0.9231\n",
      "Epoch: 893 | train_loss: 0.2524 | train_acc: 0.9249 | test_loss: 0.3530 | test_acc: 0.9244\n",
      "Epoch: 894 | train_loss: 0.2419 | train_acc: 0.9199 | test_loss: 0.3694 | test_acc: 0.9193\n",
      "Epoch: 895 | train_loss: 0.2246 | train_acc: 0.9280 | test_loss: 0.2137 | test_acc: 0.9173\n",
      "Epoch: 896 | train_loss: 0.2152 | train_acc: 0.9310 | test_loss: 0.2575 | test_acc: 0.9309\n",
      "Epoch: 897 | train_loss: 0.2413 | train_acc: 0.9228 | test_loss: 0.2294 | test_acc: 0.9173\n",
      "Epoch: 898 | train_loss: 0.2428 | train_acc: 0.9280 | test_loss: 0.2291 | test_acc: 0.9186\n",
      "Epoch: 899 | train_loss: 0.2357 | train_acc: 0.9188 | test_loss: 0.2670 | test_acc: 0.9244\n",
      "Epoch: 900 | train_loss: 0.2373 | train_acc: 0.9257 | test_loss: 0.3897 | test_acc: 0.9225\n",
      "Epoch: 901 | train_loss: 0.2532 | train_acc: 0.9214 | test_loss: 0.2745 | test_acc: 0.9134\n",
      "Epoch: 902 | train_loss: 0.2363 | train_acc: 0.9209 | test_loss: 0.4927 | test_acc: 0.9044\n",
      "Epoch: 903 | train_loss: 0.2391 | train_acc: 0.9186 | test_loss: 0.2693 | test_acc: 0.9173\n",
      "Epoch: 904 | train_loss: 0.3098 | train_acc: 0.9081 | test_loss: 0.2863 | test_acc: 0.9031\n",
      "Epoch: 905 | train_loss: 0.3245 | train_acc: 0.8876 | test_loss: 0.3690 | test_acc: 0.8941\n",
      "Epoch: 906 | train_loss: 0.3127 | train_acc: 0.8986 | test_loss: 0.4305 | test_acc: 0.9044\n",
      "Epoch: 907 | train_loss: 0.2749 | train_acc: 0.9088 | test_loss: 0.3818 | test_acc: 0.9134\n",
      "Epoch: 908 | train_loss: 0.2679 | train_acc: 0.9167 | test_loss: 0.2797 | test_acc: 0.8947\n",
      "Epoch: 909 | train_loss: 0.2453 | train_acc: 0.9202 | test_loss: 0.2361 | test_acc: 0.9167\n",
      "Epoch: 910 | train_loss: 0.2249 | train_acc: 0.9255 | test_loss: 0.3466 | test_acc: 0.9205\n",
      "Epoch: 911 | train_loss: 0.2582 | train_acc: 0.9176 | test_loss: 0.2259 | test_acc: 0.9063\n",
      "Epoch: 912 | train_loss: 0.2649 | train_acc: 0.9175 | test_loss: 0.2846 | test_acc: 0.9160\n",
      "Epoch: 913 | train_loss: 0.2582 | train_acc: 0.9178 | test_loss: 0.2299 | test_acc: 0.9238\n",
      "Epoch: 914 | train_loss: 0.2472 | train_acc: 0.9197 | test_loss: 0.2324 | test_acc: 0.9302\n",
      "Epoch: 915 | train_loss: 0.2848 | train_acc: 0.9121 | test_loss: 0.3268 | test_acc: 0.9050\n",
      "Epoch: 916 | train_loss: 0.2738 | train_acc: 0.9141 | test_loss: 0.2554 | test_acc: 0.9180\n",
      "Epoch: 917 | train_loss: 0.3130 | train_acc: 0.9026 | test_loss: 0.3319 | test_acc: 0.8863\n",
      "Epoch: 918 | train_loss: 0.3644 | train_acc: 0.8844 | test_loss: 0.3898 | test_acc: 0.8786\n",
      "Epoch: 919 | train_loss: 0.3560 | train_acc: 0.8829 | test_loss: 0.4175 | test_acc: 0.8702\n",
      "Epoch: 920 | train_loss: 0.3555 | train_acc: 0.8884 | test_loss: 0.3564 | test_acc: 0.8831\n",
      "Epoch: 921 | train_loss: 0.3138 | train_acc: 0.8949 | test_loss: 0.3348 | test_acc: 0.8902\n",
      "Epoch: 922 | train_loss: 0.3198 | train_acc: 0.9008 | test_loss: 0.2995 | test_acc: 0.9057\n",
      "Epoch: 923 | train_loss: 0.3209 | train_acc: 0.8931 | test_loss: 0.3139 | test_acc: 0.8921\n",
      "Epoch: 924 | train_loss: 0.3588 | train_acc: 0.8921 | test_loss: 0.3809 | test_acc: 0.8766\n",
      "Epoch: 925 | train_loss: 0.3349 | train_acc: 0.8889 | test_loss: 0.4364 | test_acc: 0.8915\n",
      "Epoch: 926 | train_loss: 0.3102 | train_acc: 0.8950 | test_loss: 0.3250 | test_acc: 0.8915\n",
      "Epoch: 927 | train_loss: 0.2885 | train_acc: 0.9039 | test_loss: 0.3486 | test_acc: 0.8999\n",
      "Epoch: 928 | train_loss: 0.2941 | train_acc: 0.9005 | test_loss: 0.2791 | test_acc: 0.8889\n",
      "Epoch: 929 | train_loss: 0.2957 | train_acc: 0.9005 | test_loss: 0.3408 | test_acc: 0.9147\n",
      "Epoch: 930 | train_loss: 0.2795 | train_acc: 0.9070 | test_loss: 0.5088 | test_acc: 0.8999\n",
      "Epoch: 931 | train_loss: 0.2772 | train_acc: 0.9105 | test_loss: 0.5427 | test_acc: 0.9070\n",
      "Epoch: 932 | train_loss: 0.2918 | train_acc: 0.9105 | test_loss: 0.2883 | test_acc: 0.8973\n",
      "Epoch: 933 | train_loss: 0.2819 | train_acc: 0.9073 | test_loss: 0.2461 | test_acc: 0.9147\n",
      "Epoch: 934 | train_loss: 0.3010 | train_acc: 0.9067 | test_loss: 0.4113 | test_acc: 0.9070\n",
      "Epoch: 935 | train_loss: 0.3219 | train_acc: 0.9049 | test_loss: 0.2540 | test_acc: 0.8928\n",
      "Epoch: 936 | train_loss: 0.3100 | train_acc: 0.9016 | test_loss: 0.3067 | test_acc: 0.8960\n",
      "Epoch: 937 | train_loss: 0.2776 | train_acc: 0.9058 | test_loss: 0.2890 | test_acc: 0.8966\n",
      "Epoch: 938 | train_loss: 0.3044 | train_acc: 0.9025 | test_loss: 0.3511 | test_acc: 0.8792\n",
      "Epoch: 939 | train_loss: 0.3111 | train_acc: 0.9013 | test_loss: 0.3337 | test_acc: 0.9018\n",
      "Epoch: 940 | train_loss: 0.3171 | train_acc: 0.9012 | test_loss: 0.3417 | test_acc: 0.8915\n",
      "Epoch: 941 | train_loss: 0.3247 | train_acc: 0.8952 | test_loss: 0.2827 | test_acc: 0.9025\n",
      "Epoch: 942 | train_loss: 0.3065 | train_acc: 0.8944 | test_loss: 0.2561 | test_acc: 0.9005\n",
      "Epoch: 943 | train_loss: 0.3415 | train_acc: 0.8926 | test_loss: 0.3451 | test_acc: 0.9005\n",
      "Epoch: 944 | train_loss: 0.3765 | train_acc: 0.8811 | test_loss: 0.4780 | test_acc: 0.8366\n",
      "Epoch: 945 | train_loss: 0.4169 | train_acc: 0.8684 | test_loss: 0.4075 | test_acc: 0.8630\n",
      "Epoch: 946 | train_loss: 0.4152 | train_acc: 0.8630 | test_loss: 0.3466 | test_acc: 0.8643\n",
      "Epoch: 947 | train_loss: 0.3799 | train_acc: 0.8674 | test_loss: 0.4462 | test_acc: 0.8676\n",
      "Epoch: 948 | train_loss: 0.3921 | train_acc: 0.8737 | test_loss: 3.0471 | test_acc: 0.1182\n",
      "Epoch: 949 | train_loss: 0.4543 | train_acc: 0.8558 | test_loss: 2.1668 | test_acc: 0.3043\n",
      "Epoch: 950 | train_loss: 0.5307 | train_acc: 0.8215 | test_loss: 1.6093 | test_acc: 0.3870\n",
      "Epoch: 951 | train_loss: 0.5549 | train_acc: 0.8072 | test_loss: 1.2021 | test_acc: 0.5097\n",
      "Epoch: 952 | train_loss: 0.5359 | train_acc: 0.8056 | test_loss: 0.5752 | test_acc: 0.7810\n",
      "Epoch: 953 | train_loss: 0.5448 | train_acc: 0.8104 | test_loss: 0.6103 | test_acc: 0.8004\n",
      "Epoch: 954 | train_loss: 0.5905 | train_acc: 0.7952 | test_loss: 0.6373 | test_acc: 0.8088\n",
      "Epoch: 955 | train_loss: 0.5835 | train_acc: 0.8064 | test_loss: 0.6181 | test_acc: 0.7868\n",
      "Epoch: 956 | train_loss: 0.6319 | train_acc: 0.7749 | test_loss: 0.5972 | test_acc: 0.7384\n",
      "Epoch: 957 | train_loss: 0.6217 | train_acc: 0.7597 | test_loss: 0.6003 | test_acc: 0.7778\n",
      "Epoch: 958 | train_loss: 0.6319 | train_acc: 0.7634 | test_loss: 1.1662 | test_acc: 0.4554\n",
      "Epoch: 959 | train_loss: 0.6148 | train_acc: 0.7621 | test_loss: 0.5735 | test_acc: 0.7442\n",
      "Epoch: 960 | train_loss: 0.6409 | train_acc: 0.7558 | test_loss: 0.5826 | test_acc: 0.7468\n",
      "Epoch: 961 | train_loss: 0.5954 | train_acc: 0.7653 | test_loss: 0.6541 | test_acc: 0.7410\n",
      "Epoch: 962 | train_loss: 0.6110 | train_acc: 0.7684 | test_loss: 0.6392 | test_acc: 0.7371\n",
      "Epoch: 963 | train_loss: 0.6337 | train_acc: 0.7710 | test_loss: 0.6798 | test_acc: 0.7623\n",
      "Epoch: 964 | train_loss: 0.6583 | train_acc: 0.7600 | test_loss: 0.9541 | test_acc: 0.6240\n",
      "Epoch: 965 | train_loss: 0.7324 | train_acc: 0.7132 | test_loss: 0.8854 | test_acc: 0.6331\n",
      "Epoch: 966 | train_loss: 0.7436 | train_acc: 0.7174 | test_loss: 0.7491 | test_acc: 0.7255\n",
      "Epoch: 967 | train_loss: 0.6837 | train_acc: 0.7421 | test_loss: 0.8203 | test_acc: 0.7190\n",
      "Epoch: 968 | train_loss: 0.6858 | train_acc: 0.7519 | test_loss: 0.8779 | test_acc: 0.7351\n",
      "Epoch: 969 | train_loss: 0.6798 | train_acc: 0.7513 | test_loss: 0.6520 | test_acc: 0.7700\n",
      "Epoch: 970 | train_loss: 0.6482 | train_acc: 0.7568 | test_loss: 0.5475 | test_acc: 0.7545\n",
      "Epoch: 971 | train_loss: 0.6645 | train_acc: 0.7676 | test_loss: 0.6071 | test_acc: 0.7733\n",
      "Epoch: 972 | train_loss: 0.6168 | train_acc: 0.7831 | test_loss: 0.6455 | test_acc: 0.7681\n",
      "Epoch: 973 | train_loss: 0.5757 | train_acc: 0.7897 | test_loss: 0.4811 | test_acc: 0.7868\n",
      "Epoch: 974 | train_loss: 0.5883 | train_acc: 0.7943 | test_loss: 0.6739 | test_acc: 0.7590\n",
      "Epoch: 975 | train_loss: 0.5978 | train_acc: 0.7812 | test_loss: 0.6560 | test_acc: 0.7842\n",
      "Epoch: 976 | train_loss: 0.6089 | train_acc: 0.7791 | test_loss: 0.6005 | test_acc: 0.7849\n",
      "Epoch: 977 | train_loss: 0.5947 | train_acc: 0.7836 | test_loss: 0.5845 | test_acc: 0.7765\n",
      "Epoch: 978 | train_loss: 0.5985 | train_acc: 0.7849 | test_loss: 0.6819 | test_acc: 0.7784\n",
      "Epoch: 979 | train_loss: 0.6050 | train_acc: 0.7826 | test_loss: 0.5448 | test_acc: 0.7965\n",
      "Epoch: 980 | train_loss: 0.5644 | train_acc: 0.7930 | test_loss: 0.6408 | test_acc: 0.7894\n",
      "Epoch: 981 | train_loss: 0.5465 | train_acc: 0.7983 | test_loss: 0.5020 | test_acc: 0.8023\n",
      "Epoch: 982 | train_loss: 0.5679 | train_acc: 0.8033 | test_loss: 0.5637 | test_acc: 0.7946\n",
      "Epoch: 983 | train_loss: 0.5470 | train_acc: 0.8047 | test_loss: 0.5868 | test_acc: 0.7959\n",
      "Epoch: 984 | train_loss: 0.5632 | train_acc: 0.8052 | test_loss: 0.4681 | test_acc: 0.7997\n",
      "Epoch: 985 | train_loss: 0.5492 | train_acc: 0.8054 | test_loss: 0.6345 | test_acc: 0.7829\n",
      "Epoch: 986 | train_loss: 0.5763 | train_acc: 0.7936 | test_loss: 0.7632 | test_acc: 0.7636\n",
      "Epoch: 987 | train_loss: 0.5838 | train_acc: 0.7955 | test_loss: 0.6927 | test_acc: 0.7862\n",
      "Epoch: 988 | train_loss: 0.5626 | train_acc: 0.8047 | test_loss: 0.5372 | test_acc: 0.7991\n",
      "Epoch: 989 | train_loss: 0.5842 | train_acc: 0.7983 | test_loss: 0.5727 | test_acc: 0.7984\n",
      "Epoch: 990 | train_loss: 0.5883 | train_acc: 0.7876 | test_loss: 0.6269 | test_acc: 0.7745\n",
      "Epoch: 991 | train_loss: 0.6291 | train_acc: 0.7645 | test_loss: 0.6080 | test_acc: 0.7519\n",
      "Epoch: 992 | train_loss: 0.6633 | train_acc: 0.7505 | test_loss: 0.7216 | test_acc: 0.7442\n",
      "Epoch: 993 | train_loss: 0.6599 | train_acc: 0.7424 | test_loss: 0.7571 | test_acc: 0.7171\n",
      "Epoch: 994 | train_loss: 0.6789 | train_acc: 0.7481 | test_loss: 0.6280 | test_acc: 0.7416\n",
      "Epoch: 995 | train_loss: 0.6797 | train_acc: 0.7361 | test_loss: 0.7481 | test_acc: 0.7061\n",
      "Epoch: 996 | train_loss: 0.7011 | train_acc: 0.7251 | test_loss: 0.6676 | test_acc: 0.7339\n",
      "Epoch: 997 | train_loss: 0.6689 | train_acc: 0.7400 | test_loss: 0.7221 | test_acc: 0.7306\n",
      "Epoch: 998 | train_loss: 0.6639 | train_acc: 0.7445 | test_loss: 0.6907 | test_acc: 0.7248\n",
      "Epoch: 999 | train_loss: 0.6849 | train_acc: 0.7448 | test_loss: 0.5634 | test_acc: 0.7416\n",
      "Epoch: 1000 | train_loss: 0.6678 | train_acc: 0.7474 | test_loss: 0.7349 | test_acc: 0.7351\n",
      "OrderedDict([('block1.bn.weight', tensor([2.2366, 2.0629, 4.0562, 1.7944, 0.3129, 3.3031, 1.5067, 2.9311],\n",
      "       device='cuda:0')), ('block1.bn.bias', tensor([-1.4491,  1.7363, -0.5180, -1.2337, -3.2259,  0.8061, -2.2758, -0.1625],\n",
      "       device='cuda:0')), ('block1.bn.running_mean', tensor([0.9513, 0.9513, 0.8279, 1.1303, 0.4433, 0.8533, 0.8279, 1.1303],\n",
      "       device='cuda:0')), ('block1.bn.running_var', tensor([20.4956, 20.4956, 23.3866, 39.1589, 14.7101, 50.8842, 23.3866, 39.1589],\n",
      "       device='cuda:0')), ('block1.bn.num_batches_tracked', tensor(13000, device='cuda:0')), ('block1.conv.weight', tensor([[[-1., -1., -1., -1., -1., -1.,  1.]],\n",
      "\n",
      "        [[-1., -1., -1., -1., -1., -1.,  1.]],\n",
      "\n",
      "        [[-1., -1.,  1.,  1.,  1.,  1.,  1.]],\n",
      "\n",
      "        [[-1., -1., -1., -1., -1., -1., -1.]],\n",
      "\n",
      "        [[ 1.,  1., -1., -1.,  1.,  1.,  1.]],\n",
      "\n",
      "        [[-1.,  1.,  1.,  1.,  1.,  1.,  1.]],\n",
      "\n",
      "        [[-1., -1.,  1.,  1.,  1.,  1.,  1.]],\n",
      "\n",
      "        [[-1., -1., -1., -1., -1., -1., -1.]]], device='cuda:0')), ('block1.prelu.weight', tensor([1.7170], device='cuda:0')), ('block2.bn.weight', tensor([ 1.3057e-02,  2.0862e+00,  1.2958e+00,  2.1263e-01,  1.4117e+00,\n",
      "         1.8444e-03,  2.8007e+00,  3.6806e-01,  1.2648e+00, -4.0849e-02,\n",
      "         1.9150e+00,  3.6947e+00,  2.3306e+00,  1.6107e-02,  1.0811e-01,\n",
      "         1.1929e-01], device='cuda:0')), ('block2.bn.bias', tensor([-1.4351,  0.2699, -2.3152,  1.6981, -0.7765,  1.1177,  0.1007, -2.0679,\n",
      "        -1.3564, -1.2139, -1.2802,  0.3025,  0.3583, -1.7992, -1.6521, -1.2511],\n",
      "       device='cuda:0')), ('block2.bn.running_mean', tensor([ 5.8708, 15.6798,  6.3937, 21.9330, 15.9517,  6.8413, 11.1142,  9.3790,\n",
      "         4.6923,  7.0189, 13.4920, 11.6320, 14.6616,  4.9083,  7.0279, 19.2966],\n",
      "       device='cuda:0')), ('block2.bn.running_var', tensor([ 13.2603,  59.1484,  11.2132,  55.2859,  25.4499,  20.6776,  41.5500,\n",
      "         34.7982,  17.3659,  16.6975, 121.7202,  27.6823,  59.0338,  10.5494,\n",
      "         53.0679, 137.8497], device='cuda:0')), ('block2.bn.num_batches_tracked', tensor(13000, device='cuda:0')), ('block2.conv.weight', tensor([[[-1.,  1., -1., -1., -1., -1., -1.],\n",
      "         [ 1., -1., -1., -1., -1., -1., -1.],\n",
      "         [-1.,  1.,  1., -1., -1., -1.,  1.],\n",
      "         [ 1.,  1.,  1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [-1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [-1.,  1.,  1., -1., -1., -1.,  1.],\n",
      "         [ 1.,  1.,  1., -1., -1., -1., -1.]],\n",
      "\n",
      "        [[-1.,  1., -1.,  1.,  1.,  1., -1.],\n",
      "         [ 1., -1., -1., -1., -1., -1., -1.],\n",
      "         [ 1., -1., -1., -1., -1.,  1.,  1.],\n",
      "         [ 1., -1., -1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.],\n",
      "         [ 1., -1., -1., -1., -1., -1.,  1.],\n",
      "         [-1., -1., -1., -1., -1., -1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.]],\n",
      "\n",
      "        [[ 1.,  1., -1., -1., -1.,  1.,  1.],\n",
      "         [-1.,  1., -1.,  1.,  1.,  1.,  1.],\n",
      "         [-1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1., -1.,  1., -1., -1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [-1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1., -1., -1., -1., -1.,  1.,  1.]],\n",
      "\n",
      "        [[-1., -1., -1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1., -1., -1., -1.,  1., -1.],\n",
      "         [ 1., -1., -1., -1., -1.,  1., -1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1., -1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  1.,  1., -1., -1.]],\n",
      "\n",
      "        [[-1.,  1., -1., -1., -1.,  1.,  1.],\n",
      "         [-1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1., -1., -1.,  1., -1.],\n",
      "         [-1., -1., -1.,  1.,  1.,  1.,  1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1., -1., -1., -1.,  1., -1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1.,  1.,  1.,  1.]],\n",
      "\n",
      "        [[ 1., -1., -1., -1., -1., -1., -1.],\n",
      "         [-1., -1.,  1., -1.,  1.,  1.,  1.],\n",
      "         [ 1., -1.,  1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1., -1.,  1.,  1.,  1.,  1.],\n",
      "         [-1., -1.,  1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1., -1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1., -1., -1.,  1.,  1.,  1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.]],\n",
      "\n",
      "        [[ 1.,  1., -1.,  1.,  1., -1., -1.],\n",
      "         [-1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "         [ 1.,  1., -1., -1., -1.,  1.,  1.],\n",
      "         [-1.,  1.,  1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1., -1., -1., -1., -1.,  1.],\n",
      "         [ 1.,  1., -1., -1., -1., -1.,  1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.]],\n",
      "\n",
      "        [[-1.,  1., -1., -1., -1., -1.,  1.],\n",
      "         [-1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [-1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [-1.,  1.,  1.,  1., -1.,  1., -1.],\n",
      "         [-1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "         [-1., -1., -1.,  1.,  1.,  1.,  1.],\n",
      "         [-1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "         [-1., -1.,  1., -1., -1.,  1.,  1.]],\n",
      "\n",
      "        [[-1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [-1., -1., -1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1., -1., -1., -1.,  1., -1.,  1.],\n",
      "         [-1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [-1., -1., -1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1., -1., -1.],\n",
      "         [-1., -1.,  1.,  1.,  1., -1.,  1.],\n",
      "         [-1., -1.,  1.,  1.,  1.,  1., -1.]],\n",
      "\n",
      "        [[ 1., -1.,  1.,  1., -1., -1.,  1.],\n",
      "         [ 1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [-1., -1., -1., -1., -1.,  1.,  1.],\n",
      "         [-1., -1., -1., -1.,  1., -1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.]],\n",
      "\n",
      "        [[ 1., -1., -1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1., -1., -1., -1., -1.,  1.],\n",
      "         [-1.,  1., -1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1., -1., -1., -1.,  1., -1., -1.],\n",
      "         [ 1.,  1.,  1., -1., -1., -1., -1.]],\n",
      "\n",
      "        [[ 1., -1.,  1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1.,  1.,  1., -1.],\n",
      "         [ 1., -1.,  1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1.,  1., -1.],\n",
      "         [ 1.,  1., -1., -1., -1., -1., -1.],\n",
      "         [-1., -1.,  1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.]],\n",
      "\n",
      "        [[-1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "         [-1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.],\n",
      "         [-1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [-1., -1.,  1.,  1., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  1., -1., -1., -1.]],\n",
      "\n",
      "        [[-1.,  1., -1., -1.,  1.,  1.,  1.],\n",
      "         [-1., -1., -1., -1., -1.,  1.,  1.],\n",
      "         [ 1., -1., -1., -1., -1.,  1.,  1.],\n",
      "         [ 1., -1., -1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "         [-1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.]],\n",
      "\n",
      "        [[-1., -1., -1., -1.,  1.,  1., -1.],\n",
      "         [-1.,  1., -1., -1.,  1.,  1.,  1.],\n",
      "         [-1.,  1.,  1.,  1.,  1., -1., -1.],\n",
      "         [ 1., -1., -1., -1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  1.,  1.,  1.,  1.]],\n",
      "\n",
      "        [[ 1., -1.,  1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1., -1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  1., -1.,  1., -1.],\n",
      "         [ 1., -1., -1., -1., -1., -1., -1.],\n",
      "         [ 1., -1., -1., -1., -1., -1., -1.]]], device='cuda:0')), ('block2.prelu.weight', tensor([-0.5336], device='cuda:0')), ('block3.bn.weight', tensor([ 1.5425,  0.0035,  1.3987,  2.7995,  0.2430, -0.1136, -0.0912,  0.1530,\n",
      "         0.2008,  1.4648,  1.8105,  1.7305,  1.6567,  2.1426,  1.3596,  0.2665,\n",
      "         0.0434,  2.4780,  0.1490, -0.0204,  1.6949,  1.9233,  2.0365,  1.8602,\n",
      "         0.1671,  2.1306,  1.2357,  0.2348,  0.0253,  1.1699,  0.2808,  2.1774],\n",
      "       device='cuda:0')), ('block3.bn.bias', tensor([-0.9744, -0.3791, -0.6280,  0.2720, -1.6853, -0.4566,  0.6999, -1.3927,\n",
      "        -1.3279, -1.2728, -1.3875,  0.6626, -1.4745, -1.0694, -0.8072, -1.4021,\n",
      "        -0.7853, -0.5234, -1.5425, -0.6904, -0.8587, -0.4526, -0.4972,  0.5477,\n",
      "        -1.2552, -0.3432, -3.1148, -1.3440, -1.9106, -1.2078, -1.4522, -1.3081],\n",
      "       device='cuda:0')), ('block3.bn.running_mean', tensor([-1.3368e+00, -5.5472e+00,  2.9073e+00,  9.2773e-01,  4.2708e+00,\n",
      "         2.2238e+00,  1.4851e+01,  1.2333e+00,  2.5601e+01,  2.5790e+01,\n",
      "         4.2816e-03,  8.4455e+00,  4.6803e+00, -4.1906e+00,  5.0156e+00,\n",
      "         1.5254e+01,  3.7436e+01,  2.5293e+01,  1.7628e+01,  1.5588e+01,\n",
      "         1.7752e+01,  1.5603e+01,  2.0608e+00,  5.0609e+01,  1.1949e+01,\n",
      "         3.6325e+01, -3.3311e+00,  1.4394e+01, -4.0462e+00, -8.1315e-01,\n",
      "         7.8575e+00, -4.2779e+00], device='cuda:0')), ('block3.bn.running_var', tensor([ 16.9600,  18.3301,  24.2992,  20.4821,  38.1478,  17.6909,  67.7306,\n",
      "         34.0972,  39.8243,  70.1188,  14.5196,  75.7594,  52.1024,   4.0587,\n",
      "         43.1925,  37.6190,  45.6774,  54.0743,  32.5166,  59.9140,  52.1695,\n",
      "         49.8604,  22.0630,  37.4161,  53.3484,  29.8538,   2.0134,  43.3921,\n",
      "          4.1814,   5.5911, 114.0998,   3.1183], device='cuda:0')), ('block3.bn.num_batches_tracked', tensor(13000, device='cuda:0')), ('block3.conv.weight', tensor([[[ 1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "         [ 1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "         ...,\n",
      "         [ 1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  ...,  1.,  1., -1.],\n",
      "         [-1.,  1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "         [-1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         ...,\n",
      "         [ 1.,  1.,  1.,  ...,  1.,  1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "         [ 1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "        [[ 1.,  1., -1.,  ..., -1.,  1.,  1.],\n",
      "         [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "         ...,\n",
      "         [-1.,  1., -1.,  ...,  1.,  1., -1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "         [ 1., -1., -1.,  ..., -1.,  1.,  1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [-1., -1., -1.,  ..., -1.,  1., -1.],\n",
      "         [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         ...,\n",
      "         [ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1.,  1.,  1.,  ...,  1.,  1., -1.],\n",
      "         [-1., -1.,  1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [ 1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "         [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  ...,  1., -1.,  1.]],\n",
      "\n",
      "        [[-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1., -1., -1.,  ...,  1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "         ...,\n",
      "         [ 1.,  1., -1.,  ..., -1.,  1.,  1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "         [-1., -1.,  1.,  ...,  1.,  1.,  1.]]], device='cuda:0')), ('block3.prelu.weight', tensor([0.2562], device='cuda:0')), ('block4.bn.weight', tensor([ 1.7890e+00, -9.6464e-02,  9.9874e-01,  1.4246e+00,  1.3063e+00,\n",
      "         1.5817e+00, -2.2604e-01,  4.9566e-01,  6.9276e-01,  1.1460e+00,\n",
      "         2.5015e-01, -6.7845e-02,  1.2383e+00,  3.3435e-02,  1.5445e+00,\n",
      "         9.9586e-01, -1.2450e-03,  1.6589e+00,  1.0054e+00,  1.8655e-01,\n",
      "         8.9671e-01, -1.3481e-01,  1.3185e+00,  8.4704e-01,  4.9953e-02,\n",
      "        -1.0050e-01, -3.1338e-02, -2.2818e-02,  5.4642e-01,  7.0493e-01,\n",
      "         9.0330e-01, -3.9755e-02], device='cuda:0')), ('block4.bn.bias', tensor([ 0.4390,  0.7948, -0.3913, -0.2263, -0.4147,  0.0035, -1.2497, -2.0045,\n",
      "         1.1749,  0.5785, -1.2306, -0.9975, -0.2653, -0.7735,  0.3741, -0.9831,\n",
      "        -0.5018,  0.0221, -0.1560, -1.1408, -1.0208, -0.5962, -0.3530,  0.5951,\n",
      "        -0.6647, -1.1292,  1.0633, -0.4390,  1.1712, -1.1262, -0.9216,  0.5984],\n",
      "       device='cuda:0')), ('block4.bn.running_mean', tensor([55.1789, 63.1980, 14.9538, 26.6168,  5.6980, 65.1006,  4.1335, 49.4649,\n",
      "        62.7853, 15.0944, 14.4854, 11.1403, 11.5910, 25.4951, 57.1209,  6.5791,\n",
      "        14.0106, 24.6989,  4.1846, 41.0773,  4.4243,  2.9558,  7.0483, 17.7089,\n",
      "        10.6593,  2.3465, 10.4746, 18.8790, 55.8631, 22.6147, 19.5256, 29.5098],\n",
      "       device='cuda:0')), ('block4.bn.running_var', tensor([135.0167, 126.4331, 158.0204, 230.0842,  87.4625, 175.8726,  33.2914,\n",
      "        133.4431, 100.1713, 122.2140,  70.5890, 101.0463, 127.0865, 180.4537,\n",
      "        200.6008,  89.7321,  69.2347, 153.5739,  31.6271, 140.1115,  34.8475,\n",
      "          8.3255,  86.5083, 170.3402,  92.0658,  16.3859, 102.3691, 127.0251,\n",
      "        140.4438, 222.4336, 172.1293, 131.5231], device='cuda:0')), ('block4.bn.num_batches_tracked', tensor(13000, device='cuda:0')), ('block4.conv.weight', tensor([[[-1., -1., -1.,  ...,  1., -1., -1.],\n",
      "         [-1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
      "         [ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "         [ 1., -1., -1.,  ...,  1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "         [ 1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1.,  1.,  ...,  1.,  1., -1.],\n",
      "         [ 1., -1.,  1.,  ...,  1., -1., -1.],\n",
      "         [-1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "         ...,\n",
      "         [-1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "         [-1.,  1.,  1.,  ...,  1., -1.,  1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1., -1., -1.,  ...,  1.,  1., -1.],\n",
      "         [ 1., -1.,  1.,  ..., -1.,  1., -1.],\n",
      "         [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "         [-1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ...,  1.,  1., -1.],\n",
      "         [-1., -1.,  1.,  ..., -1., -1.,  1.],\n",
      "         [-1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "         ...,\n",
      "         [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  ...,  1.,  1., -1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "         [ 1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "         [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1.,  1., -1.,  ..., -1.,  1.,  1.]]], device='cuda:0')), ('block4.prelu.weight', tensor([-0.0965], device='cuda:0')), ('block5.bn.weight', tensor([ 0.1652,  0.1502,  0.8058,  0.2102,  1.0052,  0.6210,  1.2721,  0.0167,\n",
      "         0.0164, -0.1531,  2.3911,  0.1768,  1.0719,  0.1406,  1.4669,  1.2813,\n",
      "         1.5741,  0.0917, -0.0230,  0.3274,  0.0796, -0.0810,  0.0091,  0.0505,\n",
      "        -0.0577,  0.9123,  0.8171,  1.0418,  1.6897,  0.0264,  0.0069,  1.5458,\n",
      "         0.6122,  0.0776,  0.5774,  1.7180,  0.8143,  1.0890,  1.1166,  1.0849,\n",
      "         1.6148, -0.0847,  1.2547,  0.0594,  1.7439,  0.9818,  0.0566,  1.7026,\n",
      "         1.7404,  1.1167,  1.8015,  1.0881,  2.2982,  0.0486, -0.0618,  0.9345,\n",
      "         0.2702, -0.0726,  0.1733,  0.0840,  1.8156,  1.5846,  0.6170,  0.1991],\n",
      "       device='cuda:0')), ('block5.bn.bias', tensor([ 1.3709,  0.6150, -0.6298,  1.2691, -1.7802, -3.1863, -0.4248, -0.8150,\n",
      "         1.1785, -0.8759, -0.0500, -0.9863,  0.0516,  0.5060, -0.2644, -1.4703,\n",
      "        -0.5383,  1.0530, -0.5253, -2.1062,  1.2579,  1.1912,  1.1680, -0.5484,\n",
      "        -0.4612, -1.4864, -2.5645, -2.1108, -2.1181,  0.3270, -0.9661,  0.6422,\n",
      "        -0.7102, -1.3652, -0.9130,  0.3363, -0.7941,  2.4366, -1.6364, -0.4613,\n",
      "         0.0519, -0.6391, -0.6642,  1.2307,  0.0910, -0.3626, -0.4522, -2.7012,\n",
      "        -0.5921, -1.2714,  0.7960, -0.8685, -1.7389,  0.9729, -0.4264,  0.4704,\n",
      "         1.5590, -0.6077, -1.3813,  1.2808, -1.3319, -1.0416,  0.7460, -1.1950],\n",
      "       device='cuda:0')), ('block5.bn.running_mean', tensor([-30.7561,  14.3238,  26.7986,  45.1184,   8.5094,   5.6520,  52.5447,\n",
      "          0.4878, -15.0475,  35.1416,  29.6955,  35.8576,  -7.2717,   4.5264,\n",
      "         32.4394,  14.5640,  10.9718,  36.5824,  48.2128,  -3.8766,   4.5683,\n",
      "         -8.8573, -22.0246,  24.7561,  -5.2560,  19.8126,  -6.7265,  -5.4082,\n",
      "         -3.6799,  24.3631,  17.3699,  -2.7810,  -6.0037, -16.3002, -10.4362,\n",
      "         10.5018,  25.8128,  41.2365,  -8.7668,  44.2126, -14.6771,  41.8890,\n",
      "         -9.5944,  -8.6172,  35.9561,  -8.7928,   7.7844, -16.2117,  30.6053,\n",
      "         34.1674,  -9.1181,  38.0456,  20.2598, -10.7983,  23.5041,  33.8283,\n",
      "         -8.7428,  11.9025,   2.3502,   0.3213, -20.0963,  49.8979,  41.5121,\n",
      "          9.6682], device='cuda:0')), ('block5.bn.running_var', tensor([122.9730,  70.4122, 173.4371,  97.2108, 106.2745, 102.8144,  96.9637,\n",
      "         45.4881, 144.3217,  57.0431, 104.6324, 101.2468,  40.5693, 220.4947,\n",
      "        124.0373,  68.3826,  59.1364,  96.5105,  92.6715,  71.9718, 147.3818,\n",
      "         95.5048, 135.1546, 104.4640,  48.4251, 141.0615,  51.6296,  69.3230,\n",
      "         37.8345,  74.2719, 167.7565,  42.3160, 179.5506,  26.4217, 310.7629,\n",
      "         81.9687, 208.8077, 290.8281,  46.2372,  80.7233,  95.6147, 104.1876,\n",
      "         75.8099, 108.8793,  65.1572, 143.9766,  85.9220,  50.1405,  59.9371,\n",
      "        142.1177,  70.6733, 162.8928, 188.6317,  55.5951,  53.5336,  48.7670,\n",
      "         41.7147,  68.6104,  38.1130, 118.1332,  78.1848,  75.7749, 152.4557,\n",
      "        129.8609], device='cuda:0')), ('block5.bn.num_batches_tracked', tensor(13000, device='cuda:0')), ('block5.conv.weight', tensor([[[-1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [ 1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         ...,\n",
      "         [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "         [-1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "         ...,\n",
      "         [ 1., -1.,  1.,  ...,  1., -1.,  1.],\n",
      "         [-1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  ...,  1., -1., -1.]],\n",
      "\n",
      "        [[ 1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "         [-1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
      "         ...,\n",
      "         [-1., -1.,  1.,  ..., -1., -1., -1.],\n",
      "         [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1.,  1., -1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
      "         [ 1.,  1., -1.,  ..., -1., -1.,  1.],\n",
      "         ...,\n",
      "         [ 1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "         [ 1., -1., -1.,  ...,  1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1.,  1.,  1.]],\n",
      "\n",
      "        [[-1., -1.,  1.,  ...,  1.,  1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "         ...,\n",
      "         [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1., -1., -1.,  ...,  1., -1.,  1.]],\n",
      "\n",
      "        [[-1.,  1., -1.,  ...,  1.,  1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "         ...,\n",
      "         [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "         [ 1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1., -1., -1.,  ...,  1.,  1.,  1.]]], device='cuda:0')), ('block5.prelu.weight', tensor([0.5443], device='cuda:0')), ('block6.bn.weight', tensor([0.0453, 0.0817, 0.0976, 0.1129, 0.2218], device='cuda:0')), ('block6.bn.bias', tensor([ 0.0396, -0.0601, -0.0268, -0.1468, -0.3331], device='cuda:0')), ('block6.bn.running_mean', tensor([144.1123, -26.8080, -16.8635,  60.1246,  50.7749], device='cuda:0')), ('block6.bn.running_var', tensor([228.1354,  36.3401,  59.7133, 162.3955, 355.8732], device='cuda:0')), ('block6.bn.num_batches_tracked', tensor(13000, device='cuda:0')), ('block6.conv.weight', tensor([[[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1., -1., -1.,  ...,  1., -1., -1.],\n",
      "         [ 1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "         ...,\n",
      "         [ 1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "         [ 1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1.,  1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1.,  1., -1.],\n",
      "         [ 1.,  1., -1.,  ..., -1.,  1.,  1.],\n",
      "         ...,\n",
      "         [ 1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1.,  1., -1.,  ..., -1., -1.,  1.],\n",
      "         [ 1.,  1.,  1.,  ...,  1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "         [-1.,  1., -1.,  ..., -1., -1.,  1.],\n",
      "         ...,\n",
      "         [ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "         [ 1., -1.,  1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ...,  1., -1., -1.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [-1., -1.,  1.,  ..., -1., -1.,  1.],\n",
      "         [-1., -1.,  1.,  ...,  1.,  1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [ 1., -1.,  1.,  ..., -1., -1.,  1.],\n",
      "         [-1., -1.,  1.,  ...,  1.,  1., -1.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [ 1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [ 1.,  1.,  1.,  ..., -1., -1., -1.]]], device='cuda:0')), ('block6.prelu.weight', tensor([0.3529], device='cuda:0'))])\n",
      "best_test_acc:  95.35% \t epoch:  752\n",
      "best_train_acc:  96.56% \t epoch:  853\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "best_test_acc = train(model=model,\n",
    "      train_dataloader=train_loader,\n",
    "      test_dataloader=test_loader,\n",
    "      optimizer=optimizer,\n",
    "      loss_fn=loss_fn,\n",
    "      epochs=num_epochs,\n",
    "      device=device,\n",
    "      writer=False,\n",
    "      weight_op=weightOperation,\n",
    "      classes_num = classes_num)\n",
    "\n",
    "print(\"-\" * 50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
